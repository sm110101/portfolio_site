[
  {
    "objectID": "instructions/overview.html",
    "href": "instructions/overview.html",
    "title": "Project instruction:",
    "section": "",
    "text": "Author(s): Dr. H and Gerard Pendleton Thurston the 4th\nNote: You can delete this folder and remove it from the project website once you have read and understood the instructions. It shouldn’t be part of your final submission.\nAudio instructions:\nIf you want, you can listen to the instructions:\n\n\n\n Source: Text-to-speech conversion done with Amazon Polly on AWS \nNote: These audio instructions should not be included in your final submission or repository, once you are done wiht them, please delete the files and remove them from the website."
  },
  {
    "objectID": "instructions/overview.html#python-package-optional",
    "href": "instructions/overview.html#python-package-optional",
    "title": "Project instruction:",
    "section": "Python package (optional)",
    "text": "Python package (optional)\nThis is an optional component of the project, if you’d like, you can create a dedicated Python package for your project. The source folder for this package should be included in the root of your repository, and it can be imported into your processing scripts used in the various technical-details sections. If you create a package, it should be well-documented, with an additional tab on the navigation bar for the package documentation.\nWhile a package would typically have its own GitHub repository, for this project, please include it within the same repository.\nThe skeleton for the package is not provided in the repo, but you can recycle what you created in past assignments."
  },
  {
    "objectID": "instructions/overview.html#select-a-broad-topic-area",
    "href": "instructions/overview.html#select-a-broad-topic-area",
    "title": "Project instruction:",
    "section": "Select a broad topic area",
    "text": "Select a broad topic area\nStart by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/overview.html#narrow-your-focus",
    "href": "instructions/overview.html#narrow-your-focus",
    "title": "Project instruction:",
    "section": "Narrow your focus",
    "text": "Narrow your focus\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/overview.html#data-science-questions",
    "href": "instructions/overview.html#data-science-questions",
    "title": "Project instruction:",
    "section": "Data Science Questions",
    "text": "Data Science Questions\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/overview.html#repository-setup",
    "href": "instructions/overview.html#repository-setup",
    "title": "Project instruction:",
    "section": "Repository Setup",
    "text": "Repository Setup\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/overview.html#expectations-for-github-usage",
    "href": "instructions/overview.html#expectations-for-github-usage",
    "title": "Project instruction:",
    "section": "Expectations for GitHub Usage",
    "text": "Expectations for GitHub Usage\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n1. Use a Logical Repository Structure\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n2. Commit Regularly\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n3. Data Storage\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n4. Syncing with GU Domains\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n5. Code Documentation\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "href": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "title": "Project instruction:",
    "section": "Collaboration in Groups (If Applicable)",
    "text": "Collaboration in Groups (If Applicable)\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/overview.html#website-development",
    "href": "instructions/overview.html#website-development",
    "title": "Project instruction:",
    "section": "Website Development",
    "text": "Website Development\nIt is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/overview.html#website-hosting",
    "href": "instructions/overview.html#website-hosting",
    "title": "Project instruction:",
    "section": "Website Hosting",
    "text": "Website Hosting\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/overview.html#the-two-audiences",
    "href": "instructions/overview.html#the-two-audiences",
    "title": "Project instruction:",
    "section": "The two audiences",
    "text": "The two audiences\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/overview.html#get-started-early",
    "href": "instructions/overview.html#get-started-early",
    "title": "Project instruction:",
    "section": "Get started early",
    "text": "Get started early\nRemember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/overview.html#graduate-level-work",
    "href": "instructions/overview.html#graduate-level-work",
    "title": "Project instruction:",
    "section": "Graduate level work",
    "text": "Graduate level work\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "Project instruction:",
    "section": "The intersection of skills and domain knowledge",
    "text": "The intersection of skills and domain knowledge\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/overview.html#what-is-impact",
    "href": "instructions/overview.html#what-is-impact",
    "title": "Project instruction:",
    "section": "What is “impact”?",
    "text": "What is “impact”?\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/overview.html#what-makes-a-good-research-project",
    "href": "instructions/overview.html#what-makes-a-good-research-project",
    "title": "Project instruction:",
    "section": "What makes a good research project?",
    "text": "What makes a good research project?\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/overview.html#time-management",
    "href": "instructions/overview.html#time-management",
    "title": "Project instruction:",
    "section": "Time management",
    "text": "Time management\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/overview.html#visualization-guidelines",
    "href": "instructions/overview.html#visualization-guidelines",
    "title": "Project instruction:",
    "section": "Visualization guidelines",
    "text": "Visualization guidelines\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/overview.html#coding",
    "href": "instructions/overview.html#coding",
    "title": "Project instruction:",
    "section": "Coding",
    "text": "Coding\n\nPractice First\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\nPrototype and develop on a small data set\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/overview.html#debugging",
    "href": "instructions/overview.html#debugging",
    "title": "Project instruction:",
    "section": "Debugging",
    "text": "Debugging\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/overview.html#file-types",
    "href": "instructions/overview.html#file-types",
    "title": "Project instruction:",
    "section": "File Types",
    "text": "File Types\nYou can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/overview.html#quarto-includes",
    "href": "instructions/overview.html#quarto-includes",
    "title": "Project instruction:",
    "section": "Quarto Includes",
    "text": "Quarto Includes\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\nWhy Use Quarto Includes?\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/overview.html#citation",
    "href": "instructions/overview.html#citation",
    "title": "Project instruction:",
    "section": "Citation",
    "text": "Citation\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/overview.html#acceptable-use-cases",
    "href": "instructions/overview.html#acceptable-use-cases",
    "title": "Project instruction:",
    "section": "Acceptable use cases",
    "text": "Acceptable use cases\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/overview.html#unacceptable-use-cases",
    "href": "instructions/overview.html#unacceptable-use-cases",
    "title": "Project instruction:",
    "section": "Unacceptable use cases",
    "text": "Unacceptable use cases\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\nEffect on grade\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\nPlagiarism investigation\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "technical-details/llm-usage-log.html",
    "href": "technical-details/llm-usage-log.html",
    "title": "LLM usage log",
    "section": "",
    "text": "This page can serve as a “catch-all” for LLM use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\nLLM tools were used in the following way for the tasks below"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#brainstorming",
    "href": "technical-details/llm-usage-log.html#brainstorming",
    "title": "LLM usage log",
    "section": "Brainstorming",
    "text": "Brainstorming\n\nTo create the initial idea, LLM tools were used to brainstorm ideas and provide feedback and refine the project plan."
  },
  {
    "objectID": "technical-details/llm-usage-log.html#writing",
    "href": "technical-details/llm-usage-log.html#writing",
    "title": "LLM usage log",
    "section": "Writing:",
    "text": "Writing:\n\nReformating text from bulleted lists into proses\nProofreading\nText summarization for literature review"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#code",
    "href": "technical-details/llm-usage-log.html#code",
    "title": "LLM usage log",
    "section": "Code:",
    "text": "Code:\n\nOn the Data Cleaning page, I leveraged the use of OpenAI’s GPT-4o model to help streamline the execution time of my clean_review() function. Below is the formal citation1.\nOn the EDA page, I use GPT-4o to help with formatting visualizations, including setting up a grid structure for my subplots2 and removing an unwanted blank plot in the bottom right-hand corner3"
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html",
    "href": "technical-details/unsupervised-learning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/unsupervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#what-to-address",
    "href": "technical-details/unsupervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/eda/instructions.html",
    "href": "technical-details/eda/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/eda/instructions.html#suggested-page-structure",
    "href": "technical-details/eda/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/eda/instructions.html#what-to-address",
    "href": "technical-details/eda/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/data-collection/methods.html",
    "href": "technical-details/data-collection/methods.html",
    "title": "Methods",
    "section": "",
    "text": "Methods\nIn this section, provide an overview summary of the methods used on this page. This should include a brief description of the key techniques, algorithms, tools, or processes employed in your work. Make sure to outline the approach taken for data collection, processing, analysis, or any specific technical steps relevant to the project.\nIf you are developing a package, include a reference to the relevant documentation and provide a link here for easy access. Ensure that the package details are properly documented in its dedicated section, but mentioned and connected here for a complete understanding of the methods used in this project."
  },
  {
    "objectID": "technical-details/data-collection/closing.html",
    "href": "technical-details/data-collection/closing.html",
    "title": "Summary",
    "section": "",
    "text": "In this section, I loaded in raw data in JSON form containing a corppus of over 6 million Amazon product reviews under the ‘Electronics’ category. With the help of custom functions provided in the dataset’s source repository, I parsed the data and passed it into a pandas dataframe. As a final step, I zipped the file and moved it into the data/raw-data/ directory.\n\n\nMemory and Processing Bottlenecks\nDue to the massive size of the raw data file, I had to try a bunch of new methods to successfully load it in. In the end, I elected to use a refined version of the json package called orjson. Where json is the base python library for working with JSON formatted files, orjson is a third-party library build in Rust, and optimized for procedures that require rapid serialization of large-scale JSON files.\n\n\n\nNext Section: Data Cleaning"
  },
  {
    "objectID": "technical-details/data-collection/closing.html#challenges",
    "href": "technical-details/data-collection/closing.html#challenges",
    "title": "Summary",
    "section": "",
    "text": "Memory and Processing Bottlenecks\nDue to the massive size of the raw data file, I had to try a bunch of new methods to successfully load it in. In the end, I elected to use a refined version of the json package called orjson. Where json is the base python library for working with JSON formatted files, orjson is a third-party library build in Rust, and optimized for procedures that require rapid serialization of large-scale JSON files."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#section",
    "href": "technical-details/data-collection/closing.html#section",
    "title": "Summary",
    "section": "",
    "text": "Next Section: Data Cleaning"
  },
  {
    "objectID": "report/report.html",
    "href": "report/report.html",
    "title": "Final Report",
    "section": "",
    "text": "Audio instructions:\nIf you want, you can listen to the instructions:\n\n\n\n Source: Text-to-speech conversion done with Amazon Polly on AWS \nNote: These audio instructions should not be included in your final submission or repository, once you are done wiht them, please delete the files and remove them from the website.\nThis report is designed for a non-technical audience (e.g., the general public, executives, marketing teams, or clients), focusing on high-level insights, actionable results, and visualizations to convey the impact without requiring technical knowledge. The goal is to highlight how a model affects business strategy or revenue without diving into complex methods.\n\n\n\nClear Purpose: Define the core message or objective early on.\nKnow Your Audience: Adjust language, tone, and detail based on the audience’s understanding.\nStrong Opening: Start with a hook that sets context and stakes.\nLogical Flow: Structure with a clear beginning, middle, and end.\nKey Insights: Highlight the most important points; avoid unnecessary details or jargon.\nData Support: Use data to enhance the narrative without overwhelming.\nVisuals: Incorporate charts to simplify ideas and engage the audience.\nActionable Takeaways: Conclude with recommendations or next steps.\nAuthenticity: Use storytelling to make the content engaging and relatable.\nRevise: Edit for clarity and impact, removing unnecessary content.\n\n\n\n\nThese are just examples, you can use any structure that is suitable for your project.\n\n\n\nIntroduction: Provide an accessible overview and explain the motivation and importance of the research. Example: “This study explores how climate change affects local ecosystems, vital for wildlife conservation.”\nObjective: Clearly define the research goal and relate it to real-world challenges. Example: “We aim to analyze the effects of air pollution on public health in urban areas.”\nKey Findings: Present insights without technical terms, focusing on the impact. Example: “Air pollution increases the risk of respiratory diseases by 20%.”\nMethodology Overview: Briefly explain relevant methods. Example: “We analyzed air quality data from 50 cities and surveyed 10,000 residents.”\nVisualizations: Use simple graphs and infographics to convey findings. Example: A map showing pollution levels and a bar chart of health risks.\nSocietal Implications: Highlight the broader impact. Example: “This study highlights the need for better air quality policies.”\nCall to Action: Offer recommendations based on findings. Example: “We recommend city planners invest in green spaces.”\nConclusion: Recap the main findings and societal impact. Example: “Understanding pollution’s health impacts will help create healthier cities.”\n\n\n\n\n\nExecutive Summary: Summarize key findings and their relevance to business goals. Example: “This report predicts customer churn and offers strategies to reduce churn by 15%.”\nObjective: Define the problem or question addressed. Example: “This project aims to identify the drivers of customer churn.”\nKey Insights: Present the most important, actionable insights. Example: “Customers who interact with support twice within 30 days are 25% less likely to churn.”\nVisualizations: Use clear graphs to convey key insights. Example: A bar chart showing churn likelihood based on engagement.\nBusiness Implications: Explain how findings impact business outcomes and offer specific recommendations. Example: “Focus retention efforts on low-engagement customers.”\nRecommendations: Provide clear, actionable steps with projections. Example: “Implement an automated retention campaign, reducing churn by 10%.”\nConclusion: Summarize findings and suggest next steps. Example: “Addressing churn drivers can reduce loss and improve profitability.”\nAppendix (Optional): Additional charts or explanations for further insights.\n\n\n\n\n\n\nSimplicity: Avoid jargon; focus on business-relevant insights.\nVisual Focus: Prioritize charts and graphs over dense text.\nEmphasize Impact: Always link data insights to business outcomes."
  },
  {
    "objectID": "report/report.html#guidelines-for-creating-a-good-narrative",
    "href": "report/report.html#guidelines-for-creating-a-good-narrative",
    "title": "Final Report",
    "section": "",
    "text": "Clear Purpose: Define the core message or objective early on.\nKnow Your Audience: Adjust language, tone, and detail based on the audience’s understanding.\nStrong Opening: Start with a hook that sets context and stakes.\nLogical Flow: Structure with a clear beginning, middle, and end.\nKey Insights: Highlight the most important points; avoid unnecessary details or jargon.\nData Support: Use data to enhance the narrative without overwhelming.\nVisuals: Incorporate charts to simplify ideas and engage the audience.\nActionable Takeaways: Conclude with recommendations or next steps.\nAuthenticity: Use storytelling to make the content engaging and relatable.\nRevise: Edit for clarity and impact, removing unnecessary content."
  },
  {
    "objectID": "report/report.html#report-content",
    "href": "report/report.html#report-content",
    "title": "Final Report",
    "section": "",
    "text": "These are just examples, you can use any structure that is suitable for your project.\n\n\n\nIntroduction: Provide an accessible overview and explain the motivation and importance of the research. Example: “This study explores how climate change affects local ecosystems, vital for wildlife conservation.”\nObjective: Clearly define the research goal and relate it to real-world challenges. Example: “We aim to analyze the effects of air pollution on public health in urban areas.”\nKey Findings: Present insights without technical terms, focusing on the impact. Example: “Air pollution increases the risk of respiratory diseases by 20%.”\nMethodology Overview: Briefly explain relevant methods. Example: “We analyzed air quality data from 50 cities and surveyed 10,000 residents.”\nVisualizations: Use simple graphs and infographics to convey findings. Example: A map showing pollution levels and a bar chart of health risks.\nSocietal Implications: Highlight the broader impact. Example: “This study highlights the need for better air quality policies.”\nCall to Action: Offer recommendations based on findings. Example: “We recommend city planners invest in green spaces.”\nConclusion: Recap the main findings and societal impact. Example: “Understanding pollution’s health impacts will help create healthier cities.”\n\n\n\n\n\nExecutive Summary: Summarize key findings and their relevance to business goals. Example: “This report predicts customer churn and offers strategies to reduce churn by 15%.”\nObjective: Define the problem or question addressed. Example: “This project aims to identify the drivers of customer churn.”\nKey Insights: Present the most important, actionable insights. Example: “Customers who interact with support twice within 30 days are 25% less likely to churn.”\nVisualizations: Use clear graphs to convey key insights. Example: A bar chart showing churn likelihood based on engagement.\nBusiness Implications: Explain how findings impact business outcomes and offer specific recommendations. Example: “Focus retention efforts on low-engagement customers.”\nRecommendations: Provide clear, actionable steps with projections. Example: “Implement an automated retention campaign, reducing churn by 10%.”\nConclusion: Summarize findings and suggest next steps. Example: “Addressing churn drivers can reduce loss and improve profitability.”\nAppendix (Optional): Additional charts or explanations for further insights."
  },
  {
    "objectID": "report/report.html#final-tips",
    "href": "report/report.html#final-tips",
    "title": "Final Report",
    "section": "",
    "text": "Simplicity: Avoid jargon; focus on business-relevant insights.\nVisual Focus: Prioritize charts and graphs over dense text.\nEmphasize Impact: Always link data insights to business outcomes."
  },
  {
    "objectID": "instructions/topic-selection.html",
    "href": "instructions/topic-selection.html",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena\n\n\n\n\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation.\n\n\n\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/topic-selection.html#select-a-broad-topic-area",
    "href": "instructions/topic-selection.html#select-a-broad-topic-area",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/topic-selection.html#narrow-your-focus",
    "href": "instructions/topic-selection.html#narrow-your-focus",
    "title": "Topic selection",
    "section": "",
    "text": "Narrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/topic-selection.html#data-science-questions",
    "href": "instructions/topic-selection.html#data-science-questions",
    "title": "Topic selection",
    "section": "",
    "text": "The data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/llm-usage.html",
    "href": "instructions/llm-usage.html",
    "title": "LLM usage",
    "section": "",
    "text": "We believe that the adoption of LLM tools is inevitable and will be an important skill for success in your future career. Therefore, appropriate and acceptable use of LLM tools is encouraged for this project. Use them to accelerate your workflow and learning, but not as a replacement for critical thinking and understanding. Carefully review and process their output, use them judiciously, and avoid bloating your text with LLM-generated content. Overusing these tools often degrades the quality of your work rather than enhancing it.\nRemember the following guidelines:\n\nUse common sense: If you feel like you’re doing something questionable, you probably are. A good test is to ask yourself, “Would I openly tell the professor or classmates what I’m doing right now?” If the answer is no, you’re probably doing something you shouldn’t.\nCite your LLM use cases: Always cite when and how you’ve used LLM tools. This is a requirement for the project.\nIs your use helping you grow professionally?: If your use of LLM tools is making you a more competent, efficient, and knowledgeable professional, you’re probably using them in an appropriate manner. If you’re using them as a shortcut to avoid work and gain free time, you’re using them incorrectly.\n\n\n\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1\n\n\n\n\n\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code.\n\n\n\n\n\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/llm-usage.html#citation",
    "href": "instructions/llm-usage.html#citation",
    "title": "LLM usage",
    "section": "",
    "text": "ALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/llm-usage.html#acceptable-use-cases",
    "href": "instructions/llm-usage.html#acceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "Note: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/llm-usage.html#unacceptable-use-cases",
    "href": "instructions/llm-usage.html#unacceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "DO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/expectations.html",
    "href": "instructions/expectations.html",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare.\n\n\n\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable.\n\n\n\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields.\n\n\n\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field.\n\n\n\n\n\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important.\n\n\n\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point.\n\n\n\n\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual.\n\n\n\n\n\n\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project .\n\n\n\n\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/expectations.html#get-started-early",
    "href": "instructions/expectations.html#get-started-early",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/expectations.html#graduate-level-work",
    "href": "instructions/expectations.html#graduate-level-work",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "This is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/expectations.html#what-is-impact",
    "href": "instructions/expectations.html#what-is-impact",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Impact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/expectations.html#what-makes-a-good-research-project",
    "href": "instructions/expectations.html#what-makes-a-good-research-project",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Professional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/expectations.html#time-management",
    "href": "instructions/expectations.html#time-management",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/expectations.html#visualization-guidelines",
    "href": "instructions/expectations.html#visualization-guidelines",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Visualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/expectations.html#coding",
    "href": "instructions/expectations.html#coding",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "While not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/expectations.html#debugging",
    "href": "instructions/expectations.html#debugging",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Always remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html",
    "href": "technical-details/supervised-learning/main.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "Overview\n\n\nCode\nProvide the source code used for this section of the project here.\nIf you’re using a package for code organization, you can import it at this point. However, make sure that the actual workflow steps—including data processing, analysis, and other key tasks—are conducted and clearly demonstrated on this page. The goal is to show the technical flow of your project, highlighting how the code is executed to achieve your results.\nIf relevant, link to additional documentation or external references that explain any complex components. This section should give readers a clear view of how the project is implemented from a technical perspective.\nRemember, this page is a technical narrative, NOT just a notebook with a collection of code cells, include in-line Prose, to describe what is going on."
  },
  {
    "objectID": "technical-details/data-collection/main.html#api-based-data-retrieval-using-requests-library",
    "href": "technical-details/data-collection/main.html#api-based-data-retrieval-using-requests-library",
    "title": "Data Collection",
    "section": "API-Based Data Retrieval Using requests Library**",
    "text": "API-Based Data Retrieval Using requests Library**\nThe following code uses an API-based retrieval method to download the data directly from the github repository. The pull code itself is provided by OpenAI’s GPT-4o Model3. After numerous attempts at debugging and testing the code in another environment, the LLM added a tweak in the code to disable SSL verification. While this ended up working, this is an unsafe practice, and you should always use discretion before trying it in your own analysis.\n\nimport requests #Importing requests library for  api-based retrieval\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Dataset URL\nurl = \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Electronics_5.json.gz\"\n\n# Output file\noutput_file = \"../../data/raw-data/Electronics_5.json.gz\"\n\n# Download the dataset with SSL verification disabled\nresponse = requests.get(url, stream=True, verify=False)\n\n# Check if the request was successful\nif response.status_code == 200:\n    with open(output_file, \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=1024):\n            file.write(chunk)\n    print(f\"Dataset downloaded and saved as {output_file}\")\nelse:\n    print(f\"Failed to download dataset. HTTP Status Code: {response.status_code}\")\n\nDataset downloaded and saved as ../../data/raw-data/Electronics_5.json.gz"
  },
  {
    "objectID": "technical-details/data-collection/main.html#importing-packages-and-loading-data",
    "href": "technical-details/data-collection/main.html#importing-packages-and-loading-data",
    "title": "Data Collection",
    "section": "Importing Packages and Loading Data",
    "text": "Importing Packages and Loading Data\nHere, we begin the process by loading in our data. as stated above, The initial parse() and getDF() functions are borrowed from the link above as well. However, when initially trying to load and parse this data, I ran into serious memory issues that rendered my machine unable to successfully convert the data into a dataframe. Therefore, I elected to use the orjson library over the traditional json, which cut my import time dramatically. For reference to that repository, please head here.\n\n# Importing necessary packages\nimport pandas as pd\nimport gzip\nimport orjson\n\n# Loading in the data\n\n# Defining function that parses the json file\ndef parse_orjson(path):\n    g = gzip.open(path, 'rb')\n    for l in g:\n        yield orjson.loads(l)\n\n# Defining function to load the json data into a pandas dataframe\ndef getDF_orjson(path):\n    i = 0\n    df = {}\n    for d in parse_orjson(path):\n        df[i] = d\n        i += 1\n    return pd.DataFrame.from_dict(df, orient='index')\n\n# Retrieving data\ndf = getDF_orjson('../../data/raw-data/Electronics_5.json.gz')\n\nChecking Dimensions\n\nNow that the data has been loaded, let’s check the shape\n\n\n# Printing the data shape\ndf.shape\n\n(6739590, 12)\n\n\n\n6,739,590 reviews, which coincides with the count in the original repository."
  },
  {
    "objectID": "technical-details/data-collection/main.html#zipping-data",
    "href": "technical-details/data-collection/main.html#zipping-data",
    "title": "Data Collection",
    "section": "Zipping Data",
    "text": "Zipping Data\n\nWith that out of the way, we can zip up our data and continue on with the process\n\n\ndf.to_csv('../../data/raw-data/ElectronicsReviews.csv.gz', index=False, compression='gzip')\n\nMoving Forward\n\nNow that our data has been successfully collected and loaded, we can now move on to cleaning it in the next section."
  },
  {
    "objectID": "technical-details/data-collection/main.html#challenges",
    "href": "technical-details/data-collection/main.html#challenges",
    "title": "Data Collection",
    "section": "Challenges",
    "text": "Challenges\nMemory and Processing Bottlenecks\nDue to the massive size of the raw data file, I had to try a bunch of new methods to successfully load it in. In the end, I elected to use a refined version of the json package called orjson. Where json is the base python library for working with JSON formatted files, orjson is a third-party library build in Rust, and optimized for procedures that require rapid serialization of large-scale JSON files."
  },
  {
    "objectID": "technical-details/data-collection/main.html#section",
    "href": "technical-details/data-collection/main.html#section",
    "title": "Data Collection",
    "section": "",
    "text": "Next Section: Data Cleaning"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#importing-packages",
    "href": "technical-details/data-cleaning/main.html#importing-packages",
    "title": "Data Cleaning",
    "section": "Importing Packages",
    "text": "Importing Packages\nFirst, let’s import the necessary packages.\n\n# Packages\nimport gzip # For unzipping the raw data\nimport pandas as pd # Using pandas for easier data manipulation\nimport nltk # Using nltk for its list of stopwords\nimport string # string.punctuation will be used for text cleaning\n\nimport warnings # Turning off warnings for cleaner output\n\n\nwarnings.filterwarnings('ignore') #ignoring warnings"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#loading-raw-data",
    "href": "technical-details/data-cleaning/main.html#loading-raw-data",
    "title": "Data Cleaning",
    "section": "Loading raw data",
    "text": "Loading raw data\nNow we can load in the processed CSV we constructed in the previous section.\n\n# Pathway to raw data\ndata_path = \"../../data/raw-data/ElectronicsReviews.csv.gz\"\n\n# Unzip the CSV file\nwith gzip.open(data_path, 'rb') as f:\n    # Read the CSV file into a dataframe\n    reviews_raw = pd.read_csv(f)\n\n# Display the first few lines\nreviews_raw.head(1)\n\n\n\n\n\n\n\n\noverall\nvote\nverified\nreviewTime\nreviewerID\nasin\nstyle\nreviewerName\nreviewText\nsummary\nunixReviewTime\nimage\n\n\n\n\n0\n5.0\n67\nTrue\n09 18, 1999\nAAP7PPBU72QFM\n0151004714\n{'Format:': ' Hardcover'}\nD. C. Carrad\nThis is the best novel I have read in 2 or 3 y...\nA star is born\n937612800\nNaN\n\n\n\n\n\n\n\nHere is our first look at the raw dataframe - we can observe 12 columns containing the following variables:\n\noverall: Rating\nvote: Amount of community votes given to the review. Users will often vote when they find a review helpful\nverified: Boolean variable that indicates whether an account is verified or not\nreviewTime: Time of the review (raw)\nreviewerID: ID of the reviewer\nasin: Id of the product\nstyle: Key-value object. In this case, describing product attributes\nreviewerName: Name of the reviewer\nreviewText: Raw, unprocessed text contents of the review\nsummary: The title of the user’s review\nunixReviewTime: Time of the review in unix time (Measures time “based by the number of non-leap seconds that have elapsed since 00:00:00 UTC”)1.\nimage: Image path (if any)"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#dropping-unnecessary-columns",
    "href": "technical-details/data-cleaning/main.html#dropping-unnecessary-columns",
    "title": "Data Cleaning",
    "section": "Dropping Unnecessary Columns",
    "text": "Dropping Unnecessary Columns\nFirst, lets drop the unixReviewTime, style and image columns, since they will not be used in our analysis\n\n# list of columns to drop\ndrop_cols = ['unixReviewTime', 'image', 'style']\n\n# dropping columns\nreviews_raw = reviews_raw.drop(columns=drop_cols)\n\n# ensuring correct columns were dropped\nprint(reviews_raw.columns)\n\nIndex(['overall', 'vote', 'verified', 'reviewTime', 'reviewerID', 'asin',\n       'reviewerName', 'reviewText', 'summary'],\n      dtype='object')"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#checking-for-missing-values",
    "href": "technical-details/data-cleaning/main.html#checking-for-missing-values",
    "title": "Data Cleaning",
    "section": "Checking for missing values",
    "text": "Checking for missing values\nNow lets check to see if our dataset contains any NA values\n\nprint(reviews_raw.isnull().sum())\n\noverall               0\nvote            5790916\nverified              0\nreviewTime            0\nreviewerID            0\nasin                  0\nreviewerName       1472\nreviewText         1640\nsummary             875\ndtype: int64\n\n\nIt looks like our most important column, reviewText, has 1640 null values, so let’s get rid of them.\n\n# Taking only rows or reviews_raw where reviewText is not null\nreviews_raw = reviews_raw[reviews_raw['reviewText'].notna()]\n\n# Lets make sure this works\nprint(f\"Number of NULL values in reviewText Column: {reviews_raw['reviewText'].isnull().sum()}\")\n\nNumber of NULL values in reviewText Column: 0"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#checking-data-types",
    "href": "technical-details/data-cleaning/main.html#checking-data-types",
    "title": "Data Cleaning",
    "section": "Checking data types",
    "text": "Checking data types\nBefore moving forward, let’s ensure that each of our column data types are appropriate\n\nprint(reviews_raw.dtypes)\n\noverall         float64\nvote             object\nverified           bool\nreviewTime       object\nreviewerID       object\nasin             object\nreviewerName     object\nreviewText       object\nsummary          object\ndtype: object\n\n\nLets make the following changes:\n\nConvert vote to an int object, where NA values are replaced by 0\n\n\n# Converting 'vote' to integer, while dropping columns from strings and replacing NA with 0 \nreviews_raw['vote'] = reviews_raw['vote'].replace({',': ''}, regex=True).fillna(0).astype(int)\n\nprint(f\"Vote column data type: {reviews_raw['vote'].dtype}\")\nprint(f\"Number of NULL values in vote Column: {reviews_raw['vote'].isnull().sum()}\")\n\nVote column data type: int64\nNumber of NULL values in vote Column: 0\n\n\nLets take another look at our null values\n\nprint(reviews_raw.isnull().sum())\n\noverall            0\nvote               0\nverified           0\nreviewTime         0\nreviewerID         0\nasin               0\nreviewerName    1468\nreviewText         0\nsummary          787\ndtype: int64\n\n\nAs a final step to getting rid of all Null values, lets drop instances where reviewerName and summary are Null\n\n# Taking only rows or reviewerName is not null\nreviews_raw = reviews_raw[reviews_raw['reviewerName'].notna()]\n# Now doing the same for the summary column\nreviews_raw = reviews_raw[reviews_raw['summary'].notna()]\n\n# Finally, lets print our null report\nprint(reviews_raw.isnull().sum())\n\noverall         0\nvote            0\nverified        0\nreviewTime      0\nreviewerID      0\nasin            0\nreviewerName    0\nreviewText      0\nsummary         0\ndtype: int64\n\n\nGreat! Now we are left with no more NA entries in our data."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#renaming-columns",
    "href": "technical-details/data-cleaning/main.html#renaming-columns",
    "title": "Data Cleaning",
    "section": "Renaming Columns",
    "text": "Renaming Columns\nBefore moving on to cleaning our actual text data, let’s rename overall to reviewRating and asin to productID for easier intuition and more uniformity\n\n# Renaming cols\nreviews_raw.rename(columns={\n    'overall': 'reviewRating',\n    'asin': 'productID'\n}, inplace=True)\n\n# printing result\nreviews_raw.head(1)\n\n\n\n\n\n\n\n\nreviewRating\nvote\nverified\nreviewTime\nreviewerID\nproductID\nreviewerName\nreviewText\nsummary\n\n\n\n\n0\n5.0\n67\nTrue\n09 18, 1999\nAAP7PPBU72QFM\n0151004714\nD. C. Carrad\nThis is the best novel I have read in 2 or 3 y...\nA star is born\n\n\n\n\n\n\n\nAs a last step before cleaning the text, let’s convert reviewTime to actual datetime format\n\n# Converting reviewTime from object to datetime\nreviews_raw['reviewTime'] = pd.to_datetime(reviews_raw['reviewTime'], format='%m %d, %Y', errors = 'coerce')\n\n# Ensuring this worked\nprint(f\"Data type of reviewTime column: {reviews_raw['reviewTime'].dtype}\")\n\nData type of reviewTime column: datetime64[ns]"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#cleaning-and-processing-text-data",
    "href": "technical-details/data-cleaning/main.html#cleaning-and-processing-text-data",
    "title": "Data Cleaning",
    "section": "Cleaning and Processing Text Data",
    "text": "Cleaning and Processing Text Data\nIn this section I will begin to process the text columns within this dataset. I will first remove digits from the text using python’s built-in .isdigit() function. Next, I will utilize python’s string library to create a list of punctuation marks that will be referenced when removing all punctuation from the text. Step three involves lowercasing the text using python’s built-in .lower() function. Finally I will remove stopwords in the text with the help of nltk.corpus. If you are not familiar, stopwords are terms that do not carry any semantic meaning - some examples include ‘i’, ‘me’, ‘my’, ‘myself’, ‘we’, ‘our’, ‘ours’, etc. From there. After all of this is complete, we will have a fully cleaned dataframe that is ready for analysis and modeling.\n\n# importing stopwords \nfrom nltk.corpus import stopwords\nstop_words = list(stopwords.words('english'))\n\nprint(\"STOP WORDS\")\nprint(\"=============\")\nprint(stop_words)\n\nSTOP WORDS\n=============\n['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n\n\n\n# Gathering punctuation marks to be removed\npunctuation_marks = list(string.punctuation)\n\nprint(\"PUNCTUATION MARKS\")\nprint(\"===================\")\nprint(punctuation_marks)\n\nPUNCTUATION MARKS\n===================\n['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '&lt;', '=', '&gt;', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n\n\nBelow, I define the function clean_review() which takes a string as an input and applies all of the asjustments above in the following order:\n\nRemoves digits using .isdigit()\nRemoves puncuation marks by referencing the list object punctuation_marks\nLowercases the text using python’s in-built .lower()\nRemoves stopwords using list object stop_words\n\nNote: The function2 used below was provided to me by OpenAI’s GPT-4o model. The full citation can be found at the bottom of the page. Additionally, my original function will be included in the Challenges section.\n\ndef clean_review(text):\n    # Predefine allowed characters (alphabet and space)\n    allowed_chars = set('abcdefghijklmnopqrstuvwxyz ')\n    \n    # Dropping digits and punctuation while lowercasing\n    text = ''.join(char.lower() for char in text if char.lower() in allowed_chars)\n    \n    # Splitting and removing stopwords\n    words = text.split()\n    cleaned_words = [word for word in words if word not in stop_words]\n\n    # Return the cleaned text\n    return ' '.join(cleaned_words)\n\nNow, I will apply the function above to the reviewText and summary columns\n\nreviews_raw['reviewTextClean'] = reviews_raw['reviewText'].apply(clean_review)\nreviews_raw['summaryClean'] = reviews_raw['summary'].apply(clean_review)\n\nreviews_raw[['reviewTextClean', 'summaryClean']].head()\n\n\n\n\n\n\n\n\nreviewTextClean\nsummaryClean\n\n\n\n\n0\nbest novel read years everything fiction beaut...\nstar born\n\n\n1\npages pages introspection style writers like h...\nstream consciousness novel\n\n\n2\nkind novel read time lose book days possibly w...\nim huge fan author one disappoint\n\n\n3\ngorgeous language incredible writer last life ...\nbeautiful book ever read\n\n\n4\ntaken reviews compared book leopard promised b...\ndissenting viewin part\n\n\n\n\n\n\n\nHere are the first few rows of cleaned text. It looks like everything works!"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#creating-columns-for-binary-classification",
    "href": "technical-details/data-cleaning/main.html#creating-columns-for-binary-classification",
    "title": "Data Cleaning",
    "section": "Creating Columns for Binary Classification",
    "text": "Creating Columns for Binary Classification\nFor this section, I create a new variable binary_target that I will use later on in the EDA and modeling stages. For rows where reviewRating \\(\\geq 4\\) , I assign a “positive” label, and for rows where reviewRating \\(&lt; 4\\) , I assign a “negative” label. The overall goal for this variable is to include it as a target in a binary classification model where try to predict whether a review is “positive” or “negative” based on its text contents.\n\nreviews_raw['binary_target'] = reviews_raw['reviewRating'].apply(\n    lambda x: 'positive' if x &gt;= 4 else 'negative')\n\nreviews_raw.head(1)\n\n\n\n\n\n\n\n\nreviewRating\nvote\nverified\nreviewTime\nreviewerID\nproductID\nreviewerName\nreviewText\nsummary\nreviewTextClean\nsummaryClean\nbinary_target\n\n\n\n\n0\n5.0\n67\nTrue\n1999-09-18\nAAP7PPBU72QFM\n0151004714\nD. C. Carrad\nThis is the best novel I have read in 2 or 3 y...\nA star is born\nbest novel read years everything fiction beaut...\nstar born\npositive"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#writing-data",
    "href": "technical-details/data-cleaning/main.html#writing-data",
    "title": "Data Cleaning",
    "section": "Writing Data",
    "text": "Writing Data\nAfter all of that, we can now write our cleaned dataset to a CSV file. Due to the large size of the data set, I will create two different data sets. The first file will be the entire cleaned data set, which I will write to a new directory /data/processed-data-large/, which will be included in .gitignore to reduce the size of the payload when pushing to my remote repository.\nThe second dataset, I will take a sample of 100000 observations from the dataset and write it to /data/processed-data/. That way, viewers of my remote repository will be able to see what the cleaned version of my data looks like. I will also use this data set in some cases when conducting EDA.\n\n# Creating cleaned dataframe containing all variables\nreviews = reviews_raw.copy()\n\n# Creating a dataframe with 100,000 samples from reviews\nsample_size = 100000\nreviews_short = reviews.sample(n = sample_size, random_state=5000)\n\n# Writing files\nreviews.to_csv('../../data/processed-data-large/reviews_large.csv.gz', index=False, compression='gzip')\nreviews_short.to_csv('../../data/processed-data/reviews_short.csv.gz', index = False, compression='gzip')"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#challenges",
    "href": "technical-details/data-cleaning/main.html#challenges",
    "title": "Data Cleaning",
    "section": "Challenges",
    "text": "Challenges\nText Processing Optimization\nExperienced serious bottlenecks with string-based text cleaning, so I asked chat-gpt to optimize it for me. You can find the citation below. Additionally, here is the original function that I fed into the LLM:\ndef clean_review(text):\n    #Remove digits\n    text = ''.join(char for char in text if not char.isdigit())\n    #Remove Punctuation\n    text = ''.join(char for char in text if char not in punctuation_list)\n    # Lowercase\n    text = text.lower()\n    # Remove stopwords\n    words = text.split()\n    words = [word for word in words if word not in stop_words]\n\n    # Return cleaned text\n    return ' '.join(words)\nStorage Issues\nDue to the massive size of the data, I could only save a trunacted version of the cleaned data set to the data/processed-data/ directory. Doing this made pushing the data to github easier, while also providing graders with a reference to what the whole dataset looks like. As for the full size version of the cleaned data, I stored it in a new subdirectory of the data folder: /data/processed-data-large/."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#section",
    "href": "technical-details/data-cleaning/main.html#section",
    "title": "Data Cleaning",
    "section": "",
    "text": "Next Section: Exploratory Data Analysis"
  },
  {
    "objectID": "technical-details/eda/main.html#importing-libraries",
    "href": "technical-details/eda/main.html#importing-libraries",
    "title": "Exploratory Data Analysis",
    "section": "Importing Libraries",
    "text": "Importing Libraries\nIn this section, I import all of the packages necessary for generating different types of visualizations for the data. In addition to visualization-related packages, I also bring in pandas & numpy for easy data manipulation and nltk for ngrams analysis. Finally, I will import various specialized packags for robust textual data EDA and dimension reduction.\n\n# Standard Packages\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Text-based processing and analysis packages\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Dimension reduction packages\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n# Ignoring warnings\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "technical-details/eda/main.html#loading-in-data",
    "href": "technical-details/eda/main.html#loading-in-data",
    "title": "Exploratory Data Analysis",
    "section": "Loading in data",
    "text": "Loading in data\nNow, let’s begin by loading in the cleaned data set that I constructed in the previous section:\n\nimport gzip\n\n# Pathway to raw data\ndata_path = \"../../data/processed-data/reviews_short.csv.gz\"\n\n# Unzip the CSV file\nwith gzip.open(data_path, 'rb') as f:\n    # Read the CSV file into a dataframe\n    reviews = pd.read_csv(f)\n\n\n# first few lines\nreviews.head()\n\n\n\n\n\n\n\n\nreviewRating\nvote\nverified\nreviewTime\nreviewerID\nproductID\nreviewerName\nreviewText\nsummary\nreviewTextClean\nsummaryClean\nbinary_target\n\n\n\n\n0\n5.0\n2\nFalse\n2016-06-17\nA7HY1CEDK0204\nB00I9GYG8O\nJor El\nIf you're looking for Cinema 4K capabilities o...\nFilmmakers will love this camera.\nyoure looking cinema k capabilities budget cam...\nfilmmakers love camera\npositive\n\n\n1\n2.0\n0\nFalse\n2018-07-13\nA16KA1REUFF1R8\nB01DB6BK5I\nJason A. Dangelo\n&lt;div id=\"video-block-R14IHTRCCNUS1P\" class=\"a-...\nWeb-cams from 2002 packed in a non-discrete bu...\ndiv idvideoblockrihtrccnusp classasection aspa...\nwebcams packed nondiscrete buggy package\nnegative\n\n\n2\n5.0\n0\nTrue\n2018-02-05\nA3TUCX8PIDH0JA\nB00011KM3I\nAlcerio\nGreat products and excellent services!\nFive Stars\ngreat products excellent services\nfive stars\npositive\n\n\n3\n5.0\n0\nTrue\n2014-01-16\nA255B39EEYELQ8\nB000EDOSFQ\nherbert nichols\nPriced rivaled any one sided dvd case, easy to...\nSatisfied\npriced rivaled one sided dvd case easy open dv...\nsatisfied\npositive\n\n\n4\n1.0\n5\nTrue\n2017-08-07\nA5HL1JPPIMUM3\nB01CVOLKKQ\nG. E. Roquet\nOnly made it a year, then burned up. Had to cy...\nMelted\nmade year burned cycle almost every day last m...\nmelted\nnegative"
  },
  {
    "objectID": "technical-details/eda/main.html#basic-summary-statistics",
    "href": "technical-details/eda/main.html#basic-summary-statistics",
    "title": "Exploratory Data Analysis",
    "section": "Basic Summary Statistics",
    "text": "Basic Summary Statistics\n\nSummary of quantitative columns reviewRating and vote\nmean review length (words)\nfrequency of different ratings score and binary sentiments\ndistribution of community vote numbers (excluding 0)\n\n\nLooking at reviewRating and number of community votes (vote)\nFirst, lets look at a summary of our two main nunmerical columns reviewRating and vote. When cleaning the data initially, I noticed that there are a lot of reviews that have no community votes. To address that, I will create a temporary data set called reviews_votes that drops all rows where no votes are placed. Then, we can use the pandas .describe() function, to print out the following summary statistics for each column:\n\ncount (Number of observations)\nmean (Average value)\nstd (Standard Deviation)\nmin (Minimum value)\n25th (25th percentile value)\n50th (Median value)\n75th (75th percentile value)\nmax (Maximum value)\n\n\n# Temporary data set that ignores zero vote reviews\nreview_votes = reviews[reviews['vote'] != 0]\nsummary_stats = review_votes[['reviewRating', 'vote']].describe()\n\n# Printing Summary stats\nprint(\"SUMMARY STATS FOR QUANTITATIVE COLUMNS:\")\nprint(\"========================================\")\nsummary_stats\n\nSUMMARY STATS FOR QUANTITATIVE COLUMNS:\n========================================\n\n\n\n\n\n\n\n\n\nreviewRating\nvote\n\n\n\n\ncount\n13977.000000\n13977.000000\n\n\nmean\n3.875152\n11.799099\n\n\nstd\n1.451692\n84.030018\n\n\nmin\n1.000000\n2.000000\n\n\n25%\n3.000000\n2.000000\n\n\n50%\n5.000000\n4.000000\n\n\n75%\n5.000000\n7.000000\n\n\nmax\n5.000000\n6770.000000\n\n\n\n\n\n\n\n\nInterpretation\nDue to the discrete, ordinal structure of the reviewRating column, there is not much dispersion between the median, 75th percentile, and maximum values. However, the column mean of 3.87 and median of 5 suggests that the distribution of reviewRatings is left-skewed, with over 50% of reviews containing 5-star ratings. This result echoes back to the concerns raised in the paper written by Magdelano et al. in which the researchers faced an overpresence of highly positive review ratings in their analysis of bakery reviews1. For a better understanding of how they tackled that issue, you can refer to their work (cited below), or to my literature review of the paper on the home page.\nLet us now shift our focus now to the summary statistics of the vote column. When looking at these results, one thing become abundantly clear - there is extreme variation and skewness in the distribution of community votes cast toward different reviews. For non-zero vote entries, the mean number of community votes is around 11.8 per post, with a standard deviation of 84 votes and maximum value of 6770 votes. To further quantify these characteristics, I will calculate the skewness and kurtosis of vote counts. In case you are unfamiliar with these values, “skewness” classifies the amount of assymmetry in a distribution (whether one tail is longer than the other), while kurtosis measures the “tailedness” of a distribution or what the ends of either side of the distribution look like2.\nHere is a diagram to help\n\nSource: Research Gate\nNow let’s find the actual numerical values:\n\nfrom scipy.stats import skew, kurtosis\n\nprint(f\"Skewness of Vote Counts: {skew(review_votes['vote'])}\")\nprint(f\"Kurtosis of Vote Counts: {kurtosis(review_votes['vote'])}\")\n\nSkewness of Vote Counts: 58.06515030184553\nKurtosis of Vote Counts: 4208.537902609067\n\n\nSkewness: Our value for skewness of around 58 indicates that the distribution of vote counts is extremely positively skewed - meaning vote counts for reviews are concentrated primarily around lower values. For context, a normal distribution typically has a skewness around 0.\nKurtosis: The kurtosis value of around 4208 tells us that the distribution of vote counts has very heavy tails. In other words, there are an extreme number of outliers present in our collection of community votes. For context, the standard normal distribution has a kurtosis of only 3.\n\n\nPlotting Distribution of Raw vs Clean Review Text Lengths\nLet’s move on now to looking at the distributions of the reviewText and reviewTextClean columns. The goal for this section is to gain some initial intuition into whether our text processing in the data cleaning section actually worked as intended. In order to do this, I will count the number of words and number of characters in each column and place them into two different plots showing the overlapping distributions of characters counts and word counts between the raw and cleaneed text.\n\n# Plotting distributions of word counts:\nplt.figure(figsize=(12,6))\nsns.histplot(reviews['reviewText'].apply(lambda x: len(x.split())), kde=True, bins=30, label=\"# Words in Raw Review Text\", color=\"red\", alpha=0.6)\nsns.histplot(reviews['reviewTextClean'].apply(lambda x: len(x.split())), kde=True, bins=30, label='# Words in Cleaned Review Text', color='green', alpha=0.6)\nplt.title('Distribution of Word Counts (reviewText vs reviewTextClean)')\nplt.xlabel('Word Count')\nplt.ylabel('Frequency')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation At a glance, it appears that our text processing step works as intended. In the above plot, we observe a left-ward shift in the distribution of word-length for the clean text after removing stopwords from the data. Now, lets create a similar plot showing the character counts:\n\n# Plotting distributions of character counts:\nplt.figure(figsize=(12,6))\nsns.histplot(reviews['reviewText'].apply(len), kde=True, bins=30, label='# Characters in Raw Review Text', color='red', alpha=0.6)\nsns.histplot(reviews['reviewTextClean'].apply(len), kde=True, bins=30, label='# Characters in Cleaned Review Text', color='green', alpha=0.6)\nplt.title('Distribution of Word Counts (reviewText vs reviewTextClean)')\nplt.xlabel('Word Count')\nplt.ylabel('Frequency')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nAs expected, we observe the same leftward shift in the distribution of character counts for the cleaned text"
  },
  {
    "objectID": "technical-details/eda/main.html#frequently-appearing-terms-by-review-rating",
    "href": "technical-details/eda/main.html#frequently-appearing-terms-by-review-rating",
    "title": "Exploratory Data Analysis",
    "section": "Frequently Appearing Terms by Review Rating",
    "text": "Frequently Appearing Terms by Review Rating\n\nDocument Term Matrix\nMoving forward, let’s now take a look at how different keywords relate to review ratings given in the dataset. For this section, I will leverage the CountVectorizer object from sklearn. In case you are unfamiliar, the CountVectorizer object takes in our reviews data, and converts it into a document frequency matrix where each row will represent a review, and each column represents a different word found in our every review. Each element of the matrix corresponds to the number of times word \\(i\\) appears in document \\(j\\). One of the greatest benefits to using CountVectorizer is its ability to convert unstructured text data into a meaningful numeric format. In the case below, I focus on the top 10 most common keywords for each of the five different star rating assigned to amazon electronics products.\nExample of a DTM:  \nSource: Stack Overflow \nFrom there, I will create 5 separate plots for each review score that contain bar graphs of the most frequently appearing terms. The goal for this process is to gain a better understanding of which words are more likely to appear for different product ratings.\nFor creating the plots themselves, I use the plot_top_words() function defined in dsan5000_hw-1.ipynb. However, when implementing this function, a few necessary changes had to be made. In the original function (cited above), the required inputs are a non-negative matrix factorization model, a list of words extracted from a document term matrix, and the n_top_words parameter. In my implementation, I elect to use only two inputs - a pandas dataframe, and the top_n_words parameter. The updated function then creates a document term matrix, and extracts the word list and frequencies natively using the inputed pandas dataframe. From there, the process is essentially the same - where both functions continue on by sorting the top n words, and plotting them based on the target labels. Within the function below, I leverage a popular LLM in two cases - both having to do with the overall format of the end plots. In the first case, I use OpenAI’s GPT-4o model to assist in setting up a grid structure for my subplots in order to give them a cleaner presenation on this website3. In the second case, I look for help on removing the final blank plot in the visualization4.\n\ndef plot_top_words_by_rating(reviews, n_top_words=10):\n    \"\"\"\n    This function will plot the \"n_top_words\" based on user rating\n    \"\"\"\n    # Initialize the CountVectorizer\n    vectorizer = CountVectorizer(max_features=n_top_words)\n\n    # Getting unique values for user review ratings \n    ratings = sorted(reviews['reviewRating'].unique(), reverse=True)\n    \n    ######### See [4] in references\n    # Determine grid size (2x2, 3x3, etc.)\n    n_rows = int(np.ceil(len(ratings) / 2))  # 2 columns per row\n    n_cols = 2\n    #########\n\n    # Initialize figure\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5 * n_rows))\n    axes = axes.flatten()  \n\n    # Loop through review ratings\n    for idx, rating in enumerate(ratings):\n        # Only look at reviews that have the current rating we are looping for\n        filtered_reviews = reviews[reviews['reviewRating'] == rating]['reviewTextClean']\n\n        # Fitting and transforming the reviews\n        X = vectorizer.fit_transform(filtered_reviews)\n        terms = vectorizer.get_feature_names_out() # This function extracts the actual term names from the fequency matrix\n        frequencies = X.sum(axis=0).A1 # Here, we are taking the column sums (word totals) across all of the data\n\n        # Sorting terms by frequency\n        top_features_ind = frequencies.argsort()[-n_top_words:] \n        top_features = terms[top_features_ind]  \n        weights = frequencies[top_features_ind]  \n\n        # Ploting top n words\n        ax = axes[idx]\n        ax.barh(top_features, weights, height=0.7, color=\"skyblue\") \n        ax.set_title(f\"Top {n_top_words} Words for {rating} Star Rating\", fontdict={\"fontsize\": 20})\n        ax.tick_params(axis=\"both\", which=\"major\", labelsize=14)\n        for i in \"top right left\".split():  \n            ax.spines[i].set_visible(False)\n\n\n    ######## See [5] in references\n    # Hide any unused subplots\n    for i in range(len(ratings), len(axes)):\n        fig.delaxes(axes[i])\n    ########\n\n    fig.suptitle(\"Top Words by Review Rating\", fontsize=30)\n    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.5, hspace=0.25)\n    plt.show()\n\n\nplot_top_words_by_rating(reviews, n_top_words=10)\n\n\n\n\n\n\n\n\nUnfortunately, these results are pretty unclear. There are multiple cases where the top words do not have any real relevance to their related review score. For example, in almost every plot, the top term is one… It is impossible to know what the context of this is. Similarly, we can see a high prevalence of the word “would” in many of the plots. However, there is a hint to what is going on in these plots. If you refer to the plots for 1- 2- and 3-star reviews, you can see that the word “work” appears in all three. Somewhat counterintuitive, no? It doesn’t seem very likely that the word “work” should have any association with low-rated electronics. That is exactly where the problem lies. In reality, it’s highly possible that the word “work” in most of these negative reviews follows the word “dont” (in the case of the cleaned text). Therefore, the next step in my analysis will be re-running these visualizations. However, this time, I will look at the most frequent “bigrams” or pairs of words. By doing this, I should be able to account for cases of negation in the review text.\nIn plot_top_bigrams_by_rating() below, I repeat all of the steps taken in plot_top_words_by_rating(), with the edition of the ngram_range=(2,2) parameter, which I source from an article on stack overflow. Let’s see how it goes:\n\ndef plot_top_bigrams_by_rating(reviews, n_top_bigrams=10):\n    \"\"\"\n    Plots the top `n_top_bigrams` bigrams based on user review rating.\n    \"\"\"\n    # Initialize the CountVectorizer for bigrams\n    vectorizer = CountVectorizer(max_features=n_top_bigrams, ngram_range=(2, 2))\n\n    # Getting unique values for user review ratings \n    ratings = sorted(reviews['reviewRating'].unique(), reverse=True)\n    \n    ######### See [4] in references\n    # Determine grid size (2x2, 3x3, etc.)\n    n_rows = int(np.ceil(len(ratings) / 2))  # 2 columns per row\n    n_cols = 2\n    ########\n\n    # Initialize figure\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5 * n_rows))\n    axes = axes.flatten() \n\n    # Loop through review ratings\n    for idx, rating in enumerate(ratings):\n        # Only look at reviews that have the current rating we are looping for\n        filtered_reviews = reviews[reviews['reviewRating'] == rating]['reviewTextClean']\n\n        # Fitting and transforming the reviews (for bigrams in this case)\n        X = vectorizer.fit_transform(filtered_reviews)\n        terms = vectorizer.get_feature_names_out()  # Extracting bigrams\n        frequencies = X.sum(axis=0).A1  # Here, we are taking out the column sums (bigram totals) across all of the data\n\n        # Sorting terms by frequency\n        top_features_ind = frequencies.argsort()[-n_top_bigrams:]\n        top_features = terms[top_features_ind] \n        weights = frequencies[top_features_ind] \n\n        # Plot the top n bigrams\n        ax = axes[idx]\n        ax.barh(top_features, weights, height=0.7, color=\"skyblue\")\n        ax.set_title(f\"Top {n_top_bigrams} Bigrams for {rating} Star Rating\", fontdict={\"fontsize\": 20})\n        ax.tick_params(axis=\"both\", which=\"major\", labelsize=14)\n        for i in \"top right left\".split():\n            ax.spines[i].set_visible(False)\n\n    ######### See [5] in references\n    # Hide any unused subplots\n    for i in range(len(ratings), len(axes)):\n        fig.delaxes(axes[i])\n    ##########\n\n    fig.suptitle(\"Top Bigrams by Review Rating\", fontsize=30)\n    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.5, hspace=0.25)\n    plt.show()\n\n\nplot_top_bigrams_by_rating(reviews, n_top_bigrams=10)\n\n\n\n\n\n\n\n\nMuch better! These plots provide much better context as to where certain words appear in different review scores, with bigrams like “works great” and “highly recommend” appearing frequently in highly rated products, and bigrams like “stopped working” and “waste money” appearing in lower rated reviews. However, there are still some minor issues with context - like in the case of the two-star reviews where we see a high frequency of reviews saying “would recommend”. Outside of this, it looks like we are on the right track. This particular step provides some valuable insight into our feature selection process. Specifically, when moving onto the modeling stage, I will likely elect to use bigrams over unigrams as a feature in prediction."
  },
  {
    "objectID": "technical-details/eda/main.html#sentiment-analysis-polarity-and-subjectivity",
    "href": "technical-details/eda/main.html#sentiment-analysis-polarity-and-subjectivity",
    "title": "Exploratory Data Analysis",
    "section": "Sentiment Analysis (Polarity and Subjectivity)",
    "text": "Sentiment Analysis (Polarity and Subjectivity)\nIn this section, I import the textblob library to assist in calculating the polarity and subjectivity of our cleaned review text data. In case you are not familiar, “polarity” is a continous value that falls in the range [-1, 1], where values closer to -1 indicate more of a negative tone, while values closer to 1 indicate a more positive tone5. “Subjectivity” is also a continuous value, but it falls in the range [0, 1] and quantifies the emotion or level of opinion in a piece of text. Text data with high levels of subjectivity is characterized as having greater opinionation than factual information5. For the analysis here, I will construct a few visualizations to see how these two factors behave in my review data.\n\nfrom textblob import TextBlob\n\n\n# Adding polarity column to data\nreviews['Polarity'] = reviews['reviewTextClean'].apply(lambda x: TextBlob(x).polarity)\nreviews['Subjectivity'] = reviews['reviewTextClean'].apply(lambda x: TextBlob(x).subjectivity)\n\n\nHere are the reviews with the highest and lowest polarity and subjectivity values\n\n\n#highest and lowest polarity reviews\ntop_polarity_review = reviews.nlargest(1, 'Polarity')['reviewText'].values[0]\nbottom_polarity_review = reviews.nsmallest(1, 'Polarity')['reviewText'].values[0]\n# Highest and lowest subjectivity reviews\ntop_subj_review = reviews.nlargest(1, 'Subjectivity')['reviewText'].values[0]\nbottom_subj_review = reviews.nsmallest(1, 'Subjectivity')['reviewText'].values[0]\n\n# Print the full reviews\nprint(\"POLARITY\")\nprint(\"=================\")\nprint(f\"HIGHEST: {top_polarity_review}\")\nprint(f\"LOWEST: {bottom_polarity_review} \\n\")\nprint(\"SUBJECTIVITY\")\nprint(\"=================\")\nprint(f\"HIGHEST: {top_subj_review}\")\nprint(f\"LOWEST: {bottom_subj_review} \\n\")\n\nPOLARITY\n=================\nHIGHEST: Perfect on samsung s4 mini\nLOWEST: I would never recommend this or purchase it again. The fabric is awful material. I dislike the feel of it. I literaly take it off every time I use the product. \n\nSUBJECTIVITY\n=================\nHIGHEST: Perfect on samsung s4 mini\nLOWEST: Work well! \n\n\n\n\nDistributions of Polarity and Subjectivity\nBelow, I construct two histograms showing the distribution of polarity and subjectivity across our data set\n\n# Plotting polarity Distribution \nplt.figure(figsize=(10,6))\nsns.histplot(reviews['Polarity'], kde=True, bins=30, color='blue')\nplt.title('Distribution of Polarity Scores', fontsize=16)\nplt.xlabel('Polarity', fontsize=12)\nplt.ylabel('Frequency', fontsize=12)\nplt.grid(axis='y')\nplt.show()\n\n\n\n\n\n\n\n\nWhen looking at the above plot, it’s clear that most of the reviews carry a positive sentiment on average. While there is a large collection of reviews that have a neutral tone, the overall distribution of sentiments leans more to the positive side. We can also see a disproportionately high number of reviews with positive to very positive sentiment on the right tail of the distribution. On the other hand, there are far fewer reviews that carry a negative sentiment, which again shares parallels to Magdelano’s bakery review dataset1.\n\n# Plotting Subjectivity Distribution\nplt.figure(figsize=(10, 6))\nsns.histplot(reviews['Subjectivity'], kde=True, bins=30, color='green')\nplt.title('Distribution of Subjectivity Scores', fontsize=16)\nplt.xlabel('Subjectivity', fontsize=12)\nplt.ylabel('Frequency', fontsize=12)\nplt.grid(axis='y')\nplt.show()\n\n\n\n\n\n\n\n\nLooking now at the distribution of subjectivity across electronics reviews, we see an interesting characteristic. While most reviews possess moderate subjectivity, there are extrememly high occurences of reviews with extremely high and extremely low subjectivity levels. In the case of extreme highs, it’s possible that these reviews are on average negative - written by disgruntled customers who went online to vent about their poor experience with the product. On the opposite end, reviews with extremely low subjectivity are likely more thought out and methodical, where the reviewer offers a useful documentation of their experience with the product.\n\n\nScatterplot of Polarity v Subjectivity\nTo test whether my conjecture about high subjectivity relates to negative polarity or sentiment, I construct a scatterplot between the two below. In the scatterplot, I also fill in points by their corresponding binary class label that I created in the data cleaning section. As a refresher, the binary_target variable uses the following condition:\n\n“positive” for reviewRating \\(\\geq 4\\)\n“negative” for reviewRating \\(&lt; 4\\)\n\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=reviews, x='Polarity', y='Subjectivity', hue='binary_target', alpha=0.7)\nplt.title('Polarity vs. Subjectivity', fontsize=16)\nplt.xlabel('Polarity', fontsize=12)\nplt.ylabel('Subjectivity', fontsize=12)\nplt.legend(title='Sentiment', loc='best')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nContrary to my belief above, subjectivity is not higher for more negative reviews. Instead, this plot demonstrates that subjectivity in fact tends to increase with more extreme values of polarity (regardless of which direction). In other words, reviews that are exceedingly negative and positive tend to have higher subjectivity values. As for polarity, the relationship is clear: lower polarity tends to relate to reviews classified as “negative”, and higher polarity relates to revies classified as “positive”."
  },
  {
    "objectID": "technical-details/eda/main.html#modeling-implications",
    "href": "technical-details/eda/main.html#modeling-implications",
    "title": "Exploratory Data Analysis",
    "section": "Modeling Implications",
    "text": "Modeling Implications\nPotential use of TF-IDF for single term feature\nWhile my approach to single-term modeling using a simple bag-of-words CountVectorizer approach did not prove helpful, I still want to include single terms as a feature in my modeling process. Therefore, in the modeling section, I will elect to model single terms using a TF-IDF TfidfVectorizer instead. The main reason for adopting tf-idf for single terms lies in its consideration of word-weights, which helps to penalize words that appear across all documents and place more emphasis on rarer, more semantically meaningful terms.\nBinary and Multiclass Classification\nIn the final scatterplot on this page, I compare subjectivity to polarity, while filling in points according to their binary_target class label (positive or negative). In this plot, you can see a rough separation of “positive” and “negative” reviews across the polarity axis. This discovery shows that polarity score could prove to be useful feature in predicting the binary class of each review later on in the supervised learning section. On the other hand, it is difficult to say whether polarity will be useful when running a multiclass prediction on reviewRating\nRegression\nI intend to use the values provided by textblob’s polarity function as a dependent variable in a regression model with the goal of predicting polarity for a subset of my data."
  },
  {
    "objectID": "technical-details/eda/main.html#challenges",
    "href": "technical-details/eda/main.html#challenges",
    "title": "Exploratory Data Analysis",
    "section": "Challenges",
    "text": "Challenges\nData Imbalances\nAs I outlined before, there are serious imbalances within the frequencies of different review ratings. In order to account for this, I will try to improve upon the undersampling method used in Magdelano et al1.\nNoisy Data\nIt is highly likely (even after processing) that my reviewTextClean column may typos, informal language, or otherwise irrelevant content that may impede model performance down the line."
  },
  {
    "objectID": "technical-details/eda/main.html#section",
    "href": "technical-details/eda/main.html#section",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Next Section: Unsupervised Learning"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#dimension-reduction",
    "href": "technical-details/unsupervised-learning/main.html#dimension-reduction",
    "title": "Unsupervised Learning",
    "section": "Dimension Reduction",
    "text": "Dimension Reduction\n\nPCA\nIn this section, I will use text frequency-inverse document TF-IDF as my embedding method to vectorize our reviews data. My rationale for using only TF-IDF, comes from the questionable results obtained when I used only the bag-of-words approach in the EDA section. Specifically, when looking only at single term frequency per review class, there were multiple instances where meaningless words appeared most frequently across all review scores. I was able to address this issue by instead bagging pairs of words (bigrams) instead, which yielded more intuitive results. Regardless, I choose only to use TF-IDF in this section because it considers both how often a term appears in a single review (Term Frequency), and how rare the term is accross all reviews (Inverse Document Frequency). This two-pronged approach punishes words that are common across the entire review corpus, while favoring those that are less common and therefore may have greater meaning. For further context on TF-IDF, see the equations outlined in the literature review on the Home page.\nAfter embedding the review text, I will leverage several different unsupervised learning techniques. To begin, I use two types of dimension reduction techniques to collapse the high-dimensional tf-idf matrix into a low dimensional space for easier visualization. For this, I will use Principle Components Analysis (PCA) and t-distributed Stochastic Neighbor Embedding. In case you are unfamiliar with these two topics - PCA works by identifying an axes in high-dimensional space, along which the preserved variance of the data is maximized. These so-called “principle” components are eigenvectors of the covariance matrix, and their selection (i.e. how many principle components we take) depends on the respective share of total variance preserved by their eigenvalues1.\nHere is a helpful visualization of what is going on in PCA:    Source: Builtin\n\n\nt-SNE\nOn the other hand, t-SNE takes a non-linear, probabilistic approach to dimension reduction that works in two stages. First t-SNE constructs probability distributions over different pairs of high-dimensional points, where it then assigns higher probabilities to similar points and lower probabilities to dissimilar points2. From there, t-SNE creates similar probability distributions in lower dimensional space, and shrinks the difference between the two distributions by minimizing the kullback-Leibler (KL) divergence between the two. In simple terms, the KL divergence simply measures the difference between two different probability distributions3. T-SNE also requires the use of a perplexity hyperparameter, which represents a guess as to how many close neighbors a given point should have, or the “balance between preserving the global and local structure of the data”4. Feel free to head over here for a more robust explanation of t-SNE and KL divergence.\nExample of how perplexity Influences t-SNE Results    Source: Single Cell Discoveries"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#clustering",
    "href": "technical-details/unsupervised-learning/main.html#clustering",
    "title": "Unsupervised Learning",
    "section": "Clustering",
    "text": "Clustering\nOnce the data is properly collapsed into a lower-dimensional space, I will apply several clustering methods in order to better understand how different pieces of text group together. For this, I will use K-Means Clustering, Hierarchical Clustering, and DBSCAN. The goal for using clustering methods within the context of this study is to uncover underlying patterns in text for different review rating scores.\n\nK-Means\nAs a first step, I will apply a K-Means clustering algorithm to the dimension-reduced data. The K-Means alorithm starts by randomly selecting \\(k\\) points in the dataset, where \\(k\\) is a hyperparameter that we can optimize by using the elbow method (covered below). From there, the algorithm takes these \\(k\\) centroids and calculates their distance to all other points in the data set, assigning all of the closest points to their respective centroid. For my distance metric, I elect use euclidean distance5:\n\\[\n\\text{for a point} \\ x = (x_1, x_2, ..., x_n) \\ \\text{and centroid} \\ \\mu = (\\mu_1, \\mu_2, \\mu_n) \\ \\text{their distance} \\ d(x,\\mu) = \\sqrt{\\sum_{i=1}^{n}(x_{i}-\\mu_{i})^2}\n\\]\nAfter all data points have been assigned to their initial clusters, we calculate the mean of all data points for a given cluster6:\n\\[\n\\mu_{j}^{\\text{new}} \\leftarrow \\frac{1}{|S_{j}|} \\sum_{x_{i} \\in{S_{j}}} x_{i}\n\\]\nFrom there, we repeat our disance calculation and cluster re-assignment until convergence.\nExample of K-Means Convergence    Source: Wikipedia\n\n\nHierarchical Clustering\nIn the next step of the process, I will use hierarchical clustering to help extract text groupings from the data. Unlike K-Means clustering’s reliance on centroids, hierarchical clustering uses a tree-based model of distance called a ‘dendrogram’ to analyze similarity of data points. The dendrogram is constructed through an interative process, where it progressively combines or splits up clusters based on the similarity of points within them. The process ends either when all points in the data are combined into a single cluster, or when a predefined number of clusters are formed.\nThere are two different types of hierarchical clustering, agglomerative and divisive. Here, I elect to use agglomerative or “bottom-up” hierarchical clustering to group my data points. Take for example the case when our data set a the set letters [A,B,C,D,E,F]. An agglomerative clustering model starts by treating each letter as its own cluster. In the next step, the model combines the most similar clusters. For instance, the model may merge together clusters B and C, and D and E, resulting in the new clusters [A, BC, DE, F]. From there, the model calculates cluster distances again, merging clusters DE and F, leaving us with [A, BC, DEF]. Eventually, the model will merge all clusters such that we have a single cluster [ABCDEF] (Thanks to geeksforgeeks for this example).\nSimple Example of Agglomerative Clustering:    Source: GeeksforGeeks\n\n\nDBCSCAN\nDensity-based spatial clustering of applications with noice, or DBSCAN is a density-based approach to clustering data points. I elect to use this method of clustering in my analysis because the two methods above (K-Means and Agglomerative) are geared towards finding spherical- or convex-shaped clusters (shapes that are more well-defined and less noisy). In the case of low-dimensional representations of text data, it is highly likely that clusters will not be well-defined, and therefore may require an alternative method for extracting them. That is precisely where DBSCAN comes into play. The DBSCAN algorithm requires eps and MinPts parameters, where eps “defines the neighborhood around data points,” wherein the distance between two points that are close to eachother (neighbors) approximately equal to eps7. The parameter MinPts simply defines the minimum number of data points within the eps radius, where larger data sets typically require a higher value of MinPts7.\nExample of DBSCAN Clustering"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#loading-in-data-and-libraries",
    "href": "technical-details/unsupervised-learning/main.html#loading-in-data-and-libraries",
    "title": "Unsupervised Learning",
    "section": "Loading in Data and Libraries",
    "text": "Loading in Data and Libraries\nHere, I begin by loading in all of the necessary libraries for dimension reduction and clustering. I am using the sklearn package for KMeans and DBSCAN models, scipy for agglomerative clustering, the plotly python library for displaying interactive visualizations, and pandas for data manipulation. Below, I begin by unzipping the data file we worked with in the EDA section.\n\n# Data loading and manipulation packages\nimport pandas as pd\nimport numpy as np\nimport gzip\n\n# Itertools product function to test all possible combinations of TF-IDF params for testing\nfrom itertools import product\n\n\n# Dimension reduction packages\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n# Clustering packages\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom scipy.cluster.hierarchy import linkage, dendrogram\n\n# Data visualization packages\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\n# Ignoring warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Pathway to raw data\ndata_path = \"../../data/processed-data/reviews_short.csv.gz\"\n\n# Unzip the CSV file\nwith gzip.open(data_path, 'rb') as f:\n    # Read the CSV file into a dataframe\n    reviews = pd.read_csv(f)\n\nreviews.head(1)\n\n\n\n\n\n\n\n\nreviewRating\nvote\nverified\nreviewTime\nreviewerID\nproductID\nreviewerName\nreviewText\nsummary\nreviewTextClean\nsummaryClean\nbinary_target\n\n\n\n\n0\n5.0\n2\nFalse\n2016-06-17\nA7HY1CEDK0204\nB00I9GYG8O\nJor El\nIf you're looking for Cinema 4K capabilities o...\nFilmmakers will love this camera.\nyoure looking cinema k capabilities budget cam...\nfilmmakers love camera\npositive"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#text-embedding",
    "href": "technical-details/unsupervised-learning/main.html#text-embedding",
    "title": "Unsupervised Learning",
    "section": "Text Embedding",
    "text": "Text Embedding\nIn this section, I vectorize our cleaned text data using the TfidfVectorizer object found in the sklearn library. Like I said previously, I experienced some issues relating to poor performance when applying CountVectorizer to single terms. Therefore, in this section, I elect to run the TfidfVectorizer` on single words and bigrams, as its consideration of word weights allows it to punish terms that are commonly found across all classes (review scores in this case).\n\n# Creating column of only cleaned review text\n# review_texts = reviews['reviewTextClean']\n\n\nParameter Optimization\n\ndef plot_embeddings(data, title, labels=None, hover_data=None, top_terms=None):\n    \"\"\"\n    Creates an interactive scatter plot using Plotly with hover data.\n    \n    Args:\n        data (np.array): 2D array of reduced data (e.g., PCA or t-SNE output).\n        title (str): Title of the plot.\n        labels (pd.Series or None): Numeric labels for coloring the plot (e.g., reviewRating).\n        hover_data (pd.DataFrame or None): DataFrame with hover information (optional).\n        top_terms (list of str): List of most important terms for each review.\n    \"\"\"\n    # Create a DataFrame for the plot\n    plot_df = pd.DataFrame(data, columns=['Dim 1', 'Dim 2'])\n    if labels is not None:\n        plot_df['Rating'] = labels\n    if hover_data is not None:\n        plot_df = pd.concat([plot_df, hover_data.reset_index(drop=True)], axis=1)\n    if top_terms is not None:\n        plot_df['Top Term'] = top_terms\n\n    # Generate the scatter plot\n    fig = px.scatter(\n        plot_df, \n        x='Dim 1', y='Dim 2', \n        color='Rating',  # Points are colored by the review rating\n        hover_data=['Top Term'],  # Only show the most important term on hover\n        color_continuous_scale='viridis',  # Use a color gradient for review ratings\n        title=title\n    )\n    fig.update_traces(marker=dict(size=8, opacity=0.7))\n    fig.update_layout(title=dict(x=0.5), legend=dict(orientation=\"h\", y=-0.2))\n    fig.show()\n\n\ndef reduce_and_visualize(reviews, method=\"tsne\", sample_size=1000, random_state=42, perplexity=30, intermediate_pca=True):\n    \"\"\"\n    Reduces dimensionality using PCA or t-SNE and visualizes the results with points colored by review rating.\n    \n    Args:\n        reviews (pd.DataFrame): Original dataset containing 'reviewText' and 'reviewRating'.\n        method (str): The dimensionality reduction method ('pca' or 'tsne').\n        sample_size (int): Number of rows to sample for testing.\n        random_state (int): Random state for reproducibility.\n        perplexity (int): Perplexity value for t-SNE.\n        intermediate_pca (bool): Whether to apply PCA before t-SNE.\n    \"\"\"\n    # Sample the dataset for testing\n    if sample_size &lt; len(reviews):\n        sampled_reviews = reviews.sample(n=sample_size, random_state=random_state).reset_index(drop=True)\n    else:\n        sampled_reviews = reviews.reset_index(drop=True)\n\n    # Extract the relevant columns after sampling\n    review_texts = sampled_reviews['reviewTextClean']\n    review_ratings = sampled_reviews['reviewRating']  # Correctly aligned with reviewTextClean\n    review_text = sampled_reviews['reviewText']\n\n    # Simplified parameter grid\n    param_grid = {\n        'max_features': [500],                # Single value for faster processing\n        'ngram_range': [(2, 2)],              # Focus on bigrams\n        'min_df': [1],                        # Single min_df value\n        'max_df': [0.8]                       # Single max_df value\n    }\n    \n    for max_features, ngram_range, min_df, max_df in product(\n        param_grid['max_features'], \n        param_grid['ngram_range'], \n        param_grid['min_df'], \n        param_grid['max_df']\n    ):\n        print(f\"Testing Parameters: max_features={max_features}, ngram_range={ngram_range}, min_df={min_df}, max_df={max_df}\")\n        \n        # Initialize TF-IDF\n        tfidf = TfidfVectorizer(\n            max_features=max_features,\n            ngram_range=ngram_range,\n            min_df=min_df,\n            max_df=max_df\n        )\n        \n        try:\n            # Fit and transform the data\n            X_tfidf = tfidf.fit_transform(review_texts)\n            \n            # Extract the most important term for each review\n            feature_names = np.array(tfidf.get_feature_names_out())\n            top_terms = feature_names[np.argmax(X_tfidf.toarray(), axis=1)]\n            \n            # Intermediate PCA step for t-SNE\n            if intermediate_pca and method.lower() == \"tsne\":\n                print(\"Applying intermediate PCA...\")\n                pca = PCA(n_components=50, random_state=random_state)\n                X_reduced = pca.fit_transform(X_tfidf.toarray())\n            else:\n                X_reduced = X_tfidf.toarray()\n            \n            # Dimensionality reduction\n            if method.lower() == \"pca\":\n                reducer = PCA(n_components=2, random_state=random_state)\n                reduced_data = reducer.fit_transform(X_reduced)\n                title = f\"PCA: max_features={max_features}, ngram_range={ngram_range}\"\n            \n            elif method.lower() == \"tsne\":\n                reducer = TSNE(n_components=2, perplexity=perplexity, random_state=random_state, n_iter=500)\n                reduced_data = reducer.fit_transform(X_reduced)\n                title = f\"t-SNE: max_features={max_features}, ngram_range={ngram_range}, perplexity={perplexity}\"\n            \n            else:\n                raise ValueError(\"Invalid method. Choose 'pca' or 'tsne'.\")\n            \n            # Prepare hover data\n            hover_data = pd.DataFrame({\n                'reviewText': review_text,  # Original review text\n            })\n\n            # Plot the reduced data with points colored by review rating\n            plot_embeddings(\n                reduced_data, \n                title, \n                labels=review_ratings,  # Points are colored by review ratings\n                hover_data=hover_data,\n                top_terms=top_terms\n            )\n        \n        except Exception as e:\n            print(f\"Failed for parameters: {e}\")\n\n\nreduce_and_visualize(\n    reviews=reviews,            # Pass the full dataset\n    method=\"tsne\",              # Choose \"tsne\" for t-SNE or \"pca\" for PCA\n    sample_size=2000,           # Test on a sample of 2000 reviews\n    random_state=42,            # Set a random state for reproducibility\n    perplexity=30,              # Set the perplexity for t-SNE\n    intermediate_pca=True       # Apply PCA before t-SNE\n)\n\nTesting Parameters: max_features=500, ngram_range=(2, 2), min_df=1, max_df=0.8\nApplying intermediate PCA...\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "instructions/github-usage.html",
    "href": "instructions/github-usage.html",
    "title": "GitHub",
    "section": "",
    "text": "Your project will be fully transparent, with all source code hosted on GitHub. This platform will serve as the main repository for your project code, documentation, and website. Proper organization and regular updates are key for effective collaboration and project management.\n\nIMPORTANT: Proficiency in GitHub for collaboration is a valuable addition to your resume. Being able to join a team and immediately contribute by solving problems and adding value is a highly sought-after skill. Now is the time to develop this expertise—embrace Git fully, become proficient, and graduate with a critical skill for your future career.\n\n\n\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure.\n\n\n\n\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies.\n\n\n\n\n\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/github-usage.html#repository-setup",
    "href": "instructions/github-usage.html#repository-setup",
    "title": "GitHub",
    "section": "",
    "text": "You MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/github-usage.html#expectations-for-github-usage",
    "href": "instructions/github-usage.html#expectations-for-github-usage",
    "title": "GitHub",
    "section": "",
    "text": "Your grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "href": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "title": "GitHub",
    "section": "",
    "text": "If you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/quarto-tips.html",
    "href": "instructions/quarto-tips.html",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton.\n\n\n\n\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/quarto-tips.html#file-types",
    "href": "instructions/quarto-tips.html#file-types",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/quarto-tips.html#quarto-includes",
    "href": "instructions/quarto-tips.html#quarto-includes",
    "title": "Quarto Tips",
    "section": "",
    "text": "Quarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/website-structure.html",
    "href": "instructions/website-structure.html",
    "title": "Website project structure",
    "section": "",
    "text": "Please at miniumum include the following pages in your website:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\nSupervised-learning\nLLM-usage\nProgress-log\n\n\nPlease adhere closely to this structure, for consistency accross projects.\nSub-sections can be handles as markdown headers in the respective pages.\nYou can add more pages,and if you want, you can merge EDA and unsupervised learning into one page, since the are similar. Or make these section headers in the dropdown menu, for further sub-sections creation.\nFor example:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\n\nClustering\nDimensionality Reduction\n\nSupervised-learning\n\nFeature selection\n\nregression\nclassification\n\nClassification\n\nBinary classification\nMulti-class classification\n\nRegression\n\nLLM-usage\nProgress-log\n\n\nImportant: Exactly what you put on these pages will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\nA skeleton of the recommended version of the website is provided in the github classroom repository.\n./\n├── README.md\n├── _quarto.yml\n├── assets\n│   ├── gu-logo.png\n│   ├── nature.csl\n│   └── references.bib\n├── build.sh\n├── data\n│   ├── processed-data\n│   │   └── countries_population.csv\n│   └── raw-data\n│       └── countries_population.csv\n├── index.qmd\n├── instructions\n│   ├── expectations.qmd\n│   ├── github-usage.qmd\n│   ├── llm-usage.qmd\n│   ├── overview.qmd\n│   ├── quarto-tips.qmd\n│   ├── topic-selection.qmd\n│   └── website-structure.qmd\n├── report\n│   └── report.qmd\n└── technical-details\n    ├── data-cleaning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── data-collection\n    │   ├── closing.qmd\n    │   ├── instructions.qmd\n    │   ├── main.ipynb\n    │   ├── methods.qmd\n    │   └── overview.qmd\n    ├── eda\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── llm-usage-log.qmd\n    ├── progress-log.qmd\n    ├── supervised-learning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    └── unsupervised-learning\n        ├── instructions.qmd\n        └── main.ipynb\nAlways strive to incorporate the following:\n\nStructure: Use clear headings and subheadings to break down each section of your EDA.\nClarity: Provide concise explanations for all tables and visualizations, ensuring they are easy to interpret.\nCode Links: Link to relevant code (e.g., GitHub) or embed code snippets for transparency and reproducibility.\nReproducibility: Make your EDA reproducible by providing access to the dataset, scripts, and tools you used.\nVisualization: Use visualizations to convey key insights\n\n\n\nIt is required that you build your website with Quarto.\n\n\n\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO.\n\n\n\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/website-structure.html#website-development",
    "href": "instructions/website-structure.html#website-development",
    "title": "Website project structure",
    "section": "",
    "text": "It is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/website-structure.html#website-hosting",
    "href": "instructions/website-structure.html#website-hosting",
    "title": "Website project structure",
    "section": "",
    "text": "You MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/website-structure.html#the-two-audiences",
    "href": "instructions/website-structure.html#the-two-audiences",
    "title": "Website project structure",
    "section": "",
    "text": "Knowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html",
    "href": "technical-details/data-cleaning/instructions.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "This page will cover the process of cleaning the raw data gathered in the data collection section."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#introduction",
    "href": "technical-details/data-cleaning/instructions.html#introduction",
    "title": "Data Cleaning",
    "section": "",
    "text": "This page will cover the process of cleaning the raw data gathered in the data collection section."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#suggested-page-structure",
    "href": "technical-details/data-cleaning/instructions.html#suggested-page-structure",
    "title": "Data Cleaning",
    "section": "Suggested page structure",
    "text": "Suggested page structure\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#general-comments",
    "href": "technical-details/data-cleaning/instructions.html#general-comments",
    "title": "Data Cleaning",
    "section": "General comments:",
    "text": "General comments:\n\nIterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#what-to-address",
    "href": "technical-details/data-cleaning/instructions.html#what-to-address",
    "title": "Data Cleaning",
    "section": "What to address",
    "text": "What to address\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "href": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "title": "Introduction and Motivation",
    "section": "Suggested page structure",
    "text": "Suggested page structure\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#what-to-address",
    "href": "technical-details/data-collection/instructions.html#what-to-address",
    "title": "Introduction and Motivation",
    "section": "What to address",
    "text": "What to address\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#start-collecting-data",
    "href": "technical-details/data-collection/instructions.html#start-collecting-data",
    "title": "Introduction and Motivation",
    "section": "Start collecting data:",
    "text": "Start collecting data:\nBegin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "href": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "title": "Introduction and Motivation",
    "section": "Saving the raw data",
    "text": "Saving the raw data\n\nDuring the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#requirements",
    "href": "technical-details/data-collection/instructions.html#requirements",
    "title": "Introduction and Motivation",
    "section": "Requirements:",
    "text": "Requirements:\n\nYour data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/overview.html",
    "href": "technical-details/data-collection/overview.html",
    "title": "Overview",
    "section": "",
    "text": "Overview\nIn this section, provide a high-level overview for technical staff, summarizing the key tasks and processes carried out here. Include the following elements:\n\nGoals: Clearly define the purpose of the tasks or analysis being conducted.\nMotivation: Explain the reasoning behind the work, such as solving a specific problem, improving a system, or optimizing performance.\nObjectives: Outline the specific outcomes you aim to achieve, whether it’s implementing a solution, analyzing data, or building a model.\n\nThis overview should give technical staff a clear understanding of the context and importance of the work, while guiding them on what to focus on in the details that follow."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html",
    "href": "technical-details/supervised-learning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results.\n\n\n\n\nNormalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling.\n\n\n\n\n\nModel Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used\n\n\n\n\n\nSplit Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset.\n\n\n\n\n\nBinary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc.\n\n\n\n\n\nModel Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots).\n\n\n\n\n\nResult Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#what-to-address",
    "href": "technical-details/supervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "href": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "title": "Instructions",
    "section": "",
    "text": "Normalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-selection",
    "href": "technical-details/supervised-learning/instructions.html#model-selection",
    "title": "Instructions",
    "section": "",
    "text": "Model Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "href": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "title": "Instructions",
    "section": "",
    "text": "Split Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "href": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "title": "Instructions",
    "section": "",
    "text": "Binary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#results",
    "href": "technical-details/supervised-learning/instructions.html#results",
    "title": "Instructions",
    "section": "",
    "text": "Model Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots)."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#discussion",
    "href": "technical-details/supervised-learning/instructions.html#discussion",
    "title": "Instructions",
    "section": "",
    "text": "Result Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "index.html#observing-and-predicting-the-relationship-between-customer-reviews-and-ratings-on-amazon",
    "href": "index.html#observing-and-predicting-the-relationship-between-customer-reviews-and-ratings-on-amazon",
    "title": "Welcome!",
    "section": "Observing and Predicting the Relationship Between Customer Reviews and Ratings on Amazon",
    "text": "Observing and Predicting the Relationship Between Customer Reviews and Ratings on Amazon\n\nSummarize your topic, its significance, related work, and the questions you plan to explore.\nDraft an introduction with at least 5 research questions. These may evolve as your project progresses, since data science is an iterative process.\nInclude your data science questions on this page."
  },
  {
    "objectID": "index.html#literature-review",
    "href": "index.html#literature-review",
    "title": "Welcome!",
    "section": "Literature Review",
    "text": "Literature Review\n\nA GPT-Based Approach for Sentiment Analysis and Bakery Rating Prediction\nMagdelano et al. take adopt an intruiging angle in their approach to modeling sentiment analysis in text. In the study, they work with bakery reviews collected from Google Places data. In their approach, they construct a model that uses OpenAI’s GPT-3.5-Turbo model as a “linguistic component” that takes on the task of labelling sentiments for each text input1. From there, the authors feed the outputs into a multilayer perceptron (in laymans, “a neural network that can be used to perform regression”)2 used to offer a final star-based prediction on a given review. The authors use the popular 5-point Likert Scale (Excellent, Good, Neutral, Bad, Horrible) within the context of their assigned dataset categories: Flavor (F), Variety (V), Freshness of Bread (FoB), Customer Service (CS), and Price (P)1. The authors note their initial struggles with high polarity in the data. In orther words, they encountered a disproportionate amount of five-star ratings in comparison to others. In order to account for these imbalances, the researchers used an approach called undersampling in which they selectively remove observations with the overpresent characteristic before training and testing the model1. In their conclusion, they do not cite any notable differences in model performance between the full and filtered samples. Finally the reasearchers consistently identified the computational costs of including both a LLM and MLP layer in their model.\n\n\nMachine learning-based new approach to films review\nIn this report, the authors set forth with the goal to develop a supervised learning model that accurately classifies the sentiment of a given set of movie reviews as either positive or negative. They source around 50,000 reviews from an IMDB dataset, where the amount of positive and negative sentiment-labelled records are approximately equal. For their feature extraction process, the authors elect use term frequency (TF) and term frequency-inverse document frequency (TF-IDF), where “term-frequency” refers to the number of appearances a given word or term has in a document and “inverse document frequency” is calculated by taking the log of the ratio between the total number of documents analyzed and the number of documents containing the given term. Finally, TF-IDF is the product of these two metrics3.\nIn equation form3: \\[\n\\text{TF}(t,d) = \\frac{\\text{Number of times term t appears in document d}}{\\text{Total number of terms in document d}}\n\\] \\[\n\\text{IDF}(t,D) = log_{e}\\frac{\\text{Total number of documents D in corpus}}{\\text{Number of documents containing term t}}\n\\] \\[\n\\text{TF-IDF} = TF(t,d) \\cdot IDF(t,D)\n\\]\nFrom there, the researchers test eleven different classification models including Support Vector Machines (SVM), Random Forests (RF), and K-Nearest Neighbors. In order to evaluate their results, the researchers turned to confusion matrices and F1-scores. In the end, the researchers identified the SVM with TF-IDF extracted features as having the highest overall accurary (88.33%)4\n\n\nIncorporating Topic Membership in Review Rating Prediction from Unsructured Data: A Gradient Boosting Approach\nAuthors Yang et al. leverage a bayesian network called Latent Dirichlet Allocation (LDA), as well as classical sentiment analysis in order to predict the review score given by customers of a food delivery service called JustEat5. This paper stood out for its relevance to my topic as I intend to use a similar sentiment analysis-based approach to predict amazon customer review scores across six million reviews. One of the most relevant overlaps between this article and my study comes in the nature of the relationship between the dependent variable and its features. In each case, we are extracting semantic information from a corpus of text and using it to predict a discrete ordinal variable (a 5-star review score). In their report, the authors use both sets of unstructured, topic assignment and sentiment score data as features in a regression to predict the review score. In the end, the authors are able to achieve both lower mean absolute error and root mean squared error, with 86% total accuracy in predicting review scores5."
  },
  {
    "objectID": "index.html#additional-ideas-for-things-to-include",
    "href": "index.html#additional-ideas-for-things-to-include",
    "title": "Welcome!",
    "section": "Additional Ideas for things to include",
    "text": "Additional Ideas for things to include\n\nAudience: Who is this for? Data professionals, businesses, researchers, or curious readers.\nHeadline: A captivating title introducing the data science theme (e.g., “Unlocking Insights Through Data Stories”).\nIntroduction: A brief, engaging overview of what the website offers (e.g., data-driven stories, insights, or case studies).\nQuestions You Are Addressing: What do you hope to learn?\nMotivation: Explain why this topic matters, highlighting the importance of data in solving real-world problems.\nKey Topics: List the main focus areas (e.g., machine learning, data visualization, predictive modeling).\nUse Cases/Examples: A brief teaser of compelling stories or case studies you’ve worked on.\nCall to Action: Invite visitors to explore the content, follow along, or contact you for more information.\nVisual/Infographic: Add a simple graphic or visual element to make the page more dynamic."
  },
  {
    "objectID": "technical-details/progress-log.html",
    "href": "technical-details/progress-log.html",
    "title": "Progress log",
    "section": "",
    "text": "This page serves as a progress log for my DSAN5000 final project"
  },
  {
    "objectID": "technical-details/progress-log.html#member-1-sean",
    "href": "technical-details/progress-log.html#member-1-sean",
    "title": "Progress log",
    "section": "Member-1: Sean",
    "text": "Member-1: Sean\nWeekly project contribution log:\n**12-13-2024\n\nImprove website formatting\nCorrect typos/poor writing on all pages\n\n**12-12-2024\n\nQuadruple check project guidelines to ensure proper adherence\nFinish first draft of report\n\n**12-11-2024\n\nOutline plan for report (build out structure, collect meaningful img/assets)\nFinish first draft of supervised learning page\n\n**12-10-2024\n\nOuline plan for structuring supervised learning\nFinish first draft of unsupervised learning page\n\n**12-9-2024\n\nadd a picture of review data in raw JSON format\nOutline a plan for structuring unsupervised model page\nFinish first draft of EDA page\n\n**12-8-2024\n\nBegin brainstorming ideas for EDA page\n\n12-7-2024\n\nRe-download data using webscraping method via requests libary\nFinish first draft of Data Cleaning page\n\n12-6-2024\n\nFinish first draft of Data Collection page\nFinished literature reviews for articles\n\n12-5-2024\n\nFix problems with GU Domains publishing\n\n10-16-2024\n\nDownload Amazon reviews Corpus for initial look at the data\nExplore initial topics and brainstorm methods"
  }
]