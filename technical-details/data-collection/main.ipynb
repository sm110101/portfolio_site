{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Data Collection\"\n",
    "format:\n",
    "    html: \n",
    "        code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# Overview\n",
    "\n",
    "The data from this project is open source and available via a GitHub repository. A very big thank you to the repo owner, [Jianmo Ni](https://x.com/jianmo_ni), who is a former UCSD student that compiled over 233.1 million Amazon reviews for their paper on recommendation systems[@DataArticle]. \n",
    "\n",
    "The repo owner offers this data to the public under one condition - that anyone who uses it cites their original work, which I have included in the references section below. Alternatively, this [link](https://cseweb.ucsd.edu/~jmcauley/pdfs/emnlp19a.pdf) will take you directly to the original paper that used this data. The raw data used in this report can be downloaded at the original [repo](https://nijianmo.github.io/amazon/index.html).\n",
    "\n",
    "Due to limited storage and computational capacity, I have elected to only conduct my analysis on the \"5-core\" version of Amazon Electronics reviews within the repository. The raw data is in json format, and therefore needs to be parsed and stored in a dataframe before any modeling or EDA can be done. For this process, I use the `parse()` and `getDF()` functions defined in the repository which is linked above. \n",
    "\n",
    "In case you are not familiar, the term \"5-core\" is in reference to dense subsets, and in this case it means that the data below has been filtered such that the remaining users and items have 5 reviews each. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Structure and Format\n",
    "\n",
    "Before we begin collection and cleaning, here an example of a single JSON object contained in the raw data file:\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../../xtra/multiclass-portfolio-website/images/raw_data_json.png){width=\"800px\"} \n",
    "\n",
    "Source: [The repo above](https://cseweb.ucsd.edu/~jmcauley/datasets/amazon/links.html)\n",
    "\n",
    "<br>\n",
    "\n",
    "Within this dictionary, we observe the following characteristics:\n",
    "- `overall`: Rating \n",
    "- `vote`: Amount of community votes given to the review. Users will often vote when they find a review helpful\n",
    "- `verified`: Boolean variable that indicates whether an account is verified or not\n",
    "- `reviewTime`: Time of the review (raw)\n",
    "- `reviewerID`: ID of the reviewer\n",
    "- `asin`: Id of the product\n",
    "- `style`: Key-value object. In this case, describing product attributes\n",
    "- `reviewerName`: Name of the reviewer\n",
    "- `reviewText`: Raw, unprocessed text contents of the review\n",
    "- `summary`: The title of the user's review\n",
    "- `unixReviewTime`: Time of the review in unix time (Measures time \"based by the number of non-leap seconds that have elapsed since 00:00:00 UTC\")[@UnixTime].\n",
    "- `image`: Image path (if any)\n",
    "\n",
    "The raw dataset contains a total of around 6.7 million of these entries (one for each review)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API-Based Data Retrieval Using `requests` Library**\n",
    "\n",
    "The following code uses an API-based retrieval method to download the data directly from the github repository. The pull code itself is provided by OpenAI's GPT-4o Model[@gpt4o]. After numerous attempts at debugging and testing the code in another environment, the LLM added a tweak in the code to disable SSL verification. While this ended up working, this is an unsafe practice, and you should always use discretion before trying it in your own analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #Importing requests library for  api-based retrieval\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded and saved as ../../data/raw-data/Electronics_5.json.gz\n"
     ]
    }
   ],
   "source": [
    "# Dataset URL\n",
    "url = \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Electronics_5.json.gz\"\n",
    "\n",
    "# Output file\n",
    "output_file = \"../../data/raw-data/Electronics_5.json.gz\"\n",
    "\n",
    "# Download the dataset with SSL verification disabled\n",
    "response = requests.get(url, stream=True, verify=False)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    with open(output_file, \"wb\") as file:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            file.write(chunk)\n",
    "    print(f\"Dataset downloaded and saved as {output_file}\")\n",
    "else:\n",
    "    print(f\"Failed to download dataset. HTTP Status Code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages and Loading Data\n",
    "\n",
    "Here, we begin the process by loading in our data. as stated above, The initial `parse()` and `getDF()` functions are borrowed from the link above as well. However, when initially trying to load and parse this data, I ran into serious memory issues that rendered my machine unable to successfully convert the data into a dataframe. Therefore, I elected to use the `orjson` library over the traditional `json`, which cut my import time dramatically. For reference to that repository, please head [here](https://github.com/ijl/orjson)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import orjson\n",
    "\n",
    "# Loading in the data\n",
    "\n",
    "# Defining function that parses the json file\n",
    "def parse_orjson(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield orjson.loads(l)\n",
    "\n",
    "# Defining function to load the json data into a pandas dataframe\n",
    "def getDF_orjson(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse_orjson(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "# Retrieving data\n",
    "df = getDF_orjson('../../data/raw-data/Electronics_5.json.gz')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking Dimensions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that the data has been loaded, let's check the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6739590, 12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the data shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 6,739,590 reviews, which coincides with the count in the original repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipping Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With that out of the way, we can zip up our data and continue on with the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../data/raw-data/ElectronicsReviews.csv.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Moving Forward**\n",
    "\n",
    "- Now that our data has been successfully collected and loaded, we can now move on to cleaning it in the [next](../data-cleaning/instructions.qmd) section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< include closing.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
