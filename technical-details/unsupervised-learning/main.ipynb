{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Unsupervised Learning\"\n",
    "format:\n",
    "    html: \n",
    "        code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# Overview\n",
    "\n",
    "## Dimension Reduction\n",
    "\n",
    "### PCA\n",
    "\n",
    "In this section, I will use a few different text embedding methods in order to vectorize our reviews data. Similar to what was done in the EDA section for visualizing frequent keywords for different rating scores, I will use a simple bag-of-words approach to vectorizing bigrams (pairs of words). Next, I will us text frequency-inverse document frequency (TF-IDF) to vectorize single words in the text. The rationale for using TF-IDF over bag-of-words for single terms comes from the lack of useful information provided when applying bag-of-words to single terms in the [EDA](../eda/main.ipynb) section. For a refresher on the result provided by using bag-of-words, please refer to to diagram in the [EDA](../eda/main.ipynb) section, and for further context on TF-IDF, see the equations outlined in the literature review on the [Home](../../index.qmd) page. \n",
    "\n",
    "From there, I will leverage several different unsupervised learning techniques. To begin, I use two types of dimension reduction techniques to collapse our embedded text data into a low dimensional space for easier visualization. For this, I will use Principle Components Analysis (PCA) and t-distributed Stochastic Neighbor Embedding. In case you are unfamiliar with these two topics - PCA works by identifying an axes in high-dimensional space, along which the preserved variance of the data is maximized. These so-called \"principle\" components are eigenvectors of the covariance matrix, and their selection (i.e. how many principle components we take) depends on the respective share of total variance preserved by their eigenvalues[@EigenPCA]. \n",
    "\n",
    "**Here is a helpful visualization of what is going on in PCA:**\n",
    "<br>\n",
    "![](../../xtra/multiclass-portfolio-website/images/pca.gif){width=\"600px\"} \n",
    "<br>\n",
    "Source: [Builtin](https://builtin.com/data-science/step-step-explanation-principal-component-analysis)\n",
    "\n",
    "### t-SNE\n",
    "\n",
    "On the other hand, t-SNE takes a non-linear, probabilistic approach to dimension reduction that works in two stages. First t-SNE constructs probability distributions over different pairs of high-dimensional points, where it then assigns higher probabilities to similar points and lower probabilities to dissimilar points[@WikiTSNE]. From there, creates a similar probability distributions in a lower dimensional space, and shrinks the difference between the to distributions by minimizing the kullback-Leibler (KL) divergence between the two. In simple terms, the KL divergence simply measures the difference between two different probability distributions[@WikiKL]. T-SNE also requires the use of a `perplexity` hyperparameter, which represents a guess as to how many close neighbors a given point should have, or the \"balance between preserving the global and local structure of the data\"[@perplexity]. Feel free to head over [here](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) for a more robust explanation of t-SNE and KL divergence.\n",
    "\n",
    "**Example of how `perplexity` Influences t-SNE Results**\n",
    "<br>\n",
    "![](../../xtra/multiclass-portfolio-website/images/perplexity.png){width=\"600px\"}\n",
    "<br>\n",
    "Source: [Single Cell Discoveries](https://www.scdiscoveries.com/blog/knowledge/what-is-t-sne-plot/)\n",
    "\n",
    "## Clustering\n",
    "\n",
    "Once the data is properly collapsed into a lower-dimensional space, I will apply several clustering methods in order to better understand how different pieces of text group together. For this, I will use K-Means Clustering, Hierarchical Clustering, and DBSCAN. The goal for using clustering methods within the context of this study is to uncover underlying patterns in text for different review rating scores. \n",
    "\n",
    "### K-Means\n",
    "\n",
    "As a first step, I will apply a K-Means clustering algorithm to the dimension-reduced data. The K-Means alorithm starts by randomly selecting $k$ points in the dataset, where $k$ is a hyperparameter that we can optimize by using the elbow method (covered below). From there, the algorithm takes these $k$ centroids and calculates their distance to all other points in the data set, assigning all of the closest points to their respective centroid. For my distance metric, I elect use euclidean distance[@eucdistance]: \n",
    "\n",
    "$$\n",
    "\\text{for a point} \\ x = (x_1, x_2, ..., x_n) \\ \\text{and centroid} \\ \\mu = (\\mu_1, \\mu_2, \\mu_n) \\ \\text{their distance} \\ d(x,\\mu) = \\sqrt{\\sum_{i=1}^{n}(x_{i}-\\mu_{i})^2}\n",
    "$$\n",
    "\n",
    "After all data points have been assigned to their initial clusters, we calculate the mean of all data points for a given cluster[@Week8Slides]:\n",
    "\n",
    "$$\n",
    "\\mu_{j}^{\\text{new}} \\leftarrow \\frac{1}{|S_{j}|} \\sum_{x_{i} \\in{S_{j}}} x_{i}\n",
    "$$\n",
    "\n",
    "From there, we repeat our disance calculation and cluster re-assignment until convergence.\n",
    "\n",
    "**Example of K-Means Convergence**\n",
    "<br>\n",
    "![](../../xtra/multiclass-portfolio-website/images/kmeans.gif){width=\"400px\"}\n",
    "<br>\n",
    "Source: [Wikipedia](https://commons.wikimedia.org/wiki/File:K-means_convergence.gif)\n",
    "\n",
    "### Hierarchical Clustering\n",
    "\n",
    "\n",
    "\n",
    "### Part 1: Dimensionality Reduction\n",
    "\n",
    "The objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n",
    "\n",
    "1. **PCA (Principal Component Analysis):**\n",
    "   - Apply PCA to your dataset.\n",
    "   - Determine the optimal number of principal components.\n",
    "   - Visualize the reduced-dimensional data.\n",
    "   - Analyze and interpret the results.\n",
    "\n",
    "2. **t-SNE (t-distributed Stochastic Neighbor Embedding):**\n",
    "   - Implement t-SNE on the same dataset.\n",
    "   - Experiment with different perplexity values.\n",
    "   - Visualize the t-SNE output to reveal patterns and clusters.\n",
    "   - Compare the results of t-SNE with those from PCA.\n",
    "\n",
    "3. **Evaluation and Comparison:**\n",
    "   - Evaluate the effectiveness of PCA and t-SNE in preserving data structure.\n",
    "   - Compare the visualization capabilities of both techniques.\n",
    "   - Discuss the trade-offs and scenarios where one technique may perform better than the other.\n",
    "\n",
    "### Part 2: Clustering Methods\n",
    "\n",
    "Apply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n",
    "\n",
    "1. **Clustering Methods:**\n",
    "   - Apply **K-Means**, **DBSCAN**, and **Hierarchical clustering** to your dataset.\n",
    "   - Write a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n",
    "\n",
    "2. **Results Section:**\n",
    "   - Discuss and visualize the results of each clustering analysis.\n",
    "   - Compare the performance of different clustering methods, noting any insights gained from the analysis.\n",
    "   - Visualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\n",
    "   - Use professional, labeled, and clear visualizations that support your discussion.\n",
    "\n",
    "3. **Conclusion:**\n",
    "   - Summarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code \n",
    "\n",
    "Provide the source code used for this section of the project here.\n",
    "\n",
    "If you're using a package for code organization, you can import it at this point. However, make sure that the **actual workflow steps**—including data processing, analysis, and other key tasks—are conducted and clearly demonstrated on this page. The goal is to show the technical flow of your project, highlighting how the code is executed to achieve your results.\n",
    "\n",
    "If relevant, link to additional documentation or external references that explain any complex components. This section should give readers a clear view of how the project is implemented from a technical perspective.\n",
    "\n",
    "Remember, this page is a technical narrative, NOT just a notebook with a collection of code cells, include in-line Prose, to describe what is going on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
